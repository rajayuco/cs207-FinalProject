{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autodiffpy import autodiff as ad\n",
    "from autodiffpy import autodiff_math as admath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backpropagation evaluation result: [('w0', 1), ('w1', array([3])), ('x1', array([1])), ('w2', array([2])), ('x2', array([-2]))]\n",
      "auto differentiate function evaluation result: {'w0': 1, 'w1': array([3]), 'w2': array([2]), 'x1': array([1]), 'x2': array([-2])}\n",
      "\n",
      " backpropagation evaluation result: [('w0', array([-0.36787944])), ('w1', array([-1.10363832])), ('x1', array([-0.36787944])), ('w2', array([-0.73575888])), ('x2', array([0.73575888]))]\n",
      "auto differentiate function evaluation result: {'w0': array([-0.36787944]), 'w1': array([-1.10363832]), 'w2': array([-0.73575888]), 'x1': array([-0.36787944]), 'x2': array([0.73575888])}\n",
      "\n",
      " backpropagation evaluation result: [('w0', array([0.19661193])), ('w1', array([0.5898358])), ('x1', array([0.19661193])), ('w2', array([0.39322387])), ('x2', array([-0.39322387]))]\n",
      "auto differentiate function evaluation result: {'w0': array([0.19661193]), 'w1': array([0.5898358]), 'w2': array([0.39322387]), 'x1': array([0.19661193]), 'x2': array([-0.39322387])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def exp(adobj):\n",
    "    '''Returns autodiff instance of log(x)\n",
    "\n",
    "    INPUTS\n",
    "    ==========\n",
    "    ad: autodiff instance\n",
    "\n",
    "    RETURNS\n",
    "    ==========\n",
    "    anew: autodiff instance with updated values and derivatives\n",
    "\n",
    "    EXAMPLES\n",
    "    ==========\n",
    "    >>> from autodiffpy import autodiff\n",
    "    >>> from autodiffpy import autodiff_math as admath\n",
    "    >>> x = autodiff.autodiff('x', 10)\n",
    "    >>> f1 = admath.exp(x)\n",
    "    >>> f1.val = np.exp(10)\n",
    "    '''\n",
    "    \n",
    "\n",
    "    try:\n",
    "        anew = ad.autodiff(name=adobj.name, val = np.exp(adobj.val), der = adobj.der)\n",
    "        anew.lparent = adobj\n",
    "        for key in adobj.der:\n",
    "            anew.der[key] = adobj.der[key]*anew.val\n",
    "        adobj.back_partial_der = anew.val\n",
    "        return anew\n",
    "    except TypeError:\n",
    "        print(\"Error: input should be autodiff instance only.\")\n",
    "\n",
    "\n",
    "x1 = ad.autodiff(name=\"x1\", val=3, der=1)\n",
    "x2 = ad.autodiff(name=\"x2\", val=2, der=1)\n",
    "w0 = ad.autodiff(name=\"w0\", val=2, der=1)\n",
    "w1 = ad.autodiff(name=\"w1\", val=1, der=1)\n",
    "w2 = ad.autodiff(name=\"w2\", val=-2, der=1)\n",
    "\n",
    "f =(w0+w1*x1+w2*x2)\n",
    "print('backpropagation evaluation result:', f.backprop())\n",
    "print('auto differentiate function evaluation result:', f.der)\n",
    "\n",
    "f = exp((-1)*(w0+w1*x1+w2*x2))\n",
    "print('\\n backpropagation evaluation result:', f.backprop())\n",
    "print('auto differentiate function evaluation result:', f.der)\n",
    "\n",
    "f = (1+exp((-1)*(w0+w1*x1+w2*x2)))**(-1)\n",
    "\n",
    "print('\\n backpropagation evaluation result:', f.backprop())\n",
    "print('auto differentiate function evaluation result:', f.der)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " backpropagation evaluation result: [('w0', array([ 0.95492966, -0.95492966])), ('w1', array([0.88550171, 0.88550171])), ('x1', array([-0.84559184,  0.84559184]))]\n",
      "\n",
      " auto differentiate function evaluation result: {'w0': array([ 0.76394373, -0.76394373]), 'w1': array([0.88550171, 0.88550171]), 'x1': array([-0.97640535,  0.97640535])}\n"
     ]
    }
   ],
   "source": [
    "#implementing self-test including: truediv, arcsin, arctan, arccos, subtraction etc\n",
    "x1 = ad.autodiff(name=\"x1\", val=[0.5, 0.5], der=1)\n",
    "w0 = ad.autodiff(name=\"w0\", val=0.5, der=1)\n",
    "w1 = ad.autodiff(name=\"w1\", val=[0.5, -0.5], der=1)\n",
    "\n",
    "\n",
    "f6 = admath.arctan(w0) * w1/admath.arcsin(x1)\n",
    "print('\\n backpropagation evaluation result:', f6.backprop())\n",
    "print('\\n auto differentiate function evaluation result:', f6.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# questions:\n",
    "- der should be same with backpropagation when function value are the same, or what condition?\n",
    "- when using autodiff math, there no new autodiff object generated and calculated partial derivation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_backpropagation_arc():\n",
    "    '''function for testing backpropagation'''\n",
    "    x1 = ad.autodiff(name=\"x1\", val=[0.5, 0.5], der=1)\n",
    "    w0 = ad.autodiff(name=\"w0\", val=0.5, der=1)\n",
    "    w1 = ad.autodiff(name=\"w1\", val=[0.5, -0.5], der=1)\n",
    "\n",
    "    f = admath.arctan(w0) * w1/admath.arcsin(x1)\n",
    "    assert sum(abs(f.backprop()[0][1] - [ 0.95492966, -0.95492966])) < 1E-6\n",
    "    assert sum(abs(f.backprop()[1][1] - [0.88550171, 0.88550171])) < 1E-6\n",
    "    assert sum(abs(f.backprop()[2][1] - [-0.84559184,  0.84559184])) < 1E-6  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
