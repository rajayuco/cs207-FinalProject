{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:\\\\Users\\\\wyd15\\\\Desktop\\\\Course\\\\2018Fall\\\\cs207-FinalProject\\\\autodiffpy\")\n",
    "\n",
    "#from autodiffpy \n",
    "#import autodiff_cw as ad\n",
    "import autodiff_math as admath\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:\\\\Users\\\\wyd15\\\\Download\")\n",
    "\n",
    "#from autodiffpy \n",
    "#import autodiff_cw as ad\n",
    "# import autodiff_math as admath\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autodiff_math import *\n",
    "#from autodiffpy.autodiff_math import *\n",
    "\n",
    "class autodiff():\n",
    "    def __init__(self,name,val,der=1):\n",
    "        self.name = name\n",
    "        # set val attribute\n",
    "        if isinstance(val, np.ndarray):\n",
    "            self.val = val\n",
    "        elif isinstance(val, list):\n",
    "            self.val = np.asarray(val)\n",
    "        else:\n",
    "            self.val = np.asarray([val])\n",
    "\n",
    "        if isinstance(der, np.ndarray):\n",
    "            self.der = {name:der}\n",
    "        elif isinstance(der, list):\n",
    "            self.der = {name:np.asarray(der)}\n",
    "        else:\n",
    "            self.der = {name:np.asarray([der]*self.val.shape[0])}\n",
    "\n",
    "\n",
    "        self.lparent = None\n",
    "        self.rparent = None\n",
    "\n",
    "        self.forwardpropcomplete = 'No'\n",
    "\n",
    "        self.function = None\n",
    "\n",
    "        self.back_der = None\n",
    "        self.back_partial_der = None\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"value: {self.val}\\nderivatives:{self.der}\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, autodiff) == False:\n",
    "            raise ValueError(\"Error: only autodiff instances can be compared with another.\")\n",
    "        return (self.val == other.val) and (self.der == other.der)\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        return not (self == other)\n",
    "\n",
    "    def __neg__(self):\n",
    "        \"\"\"Allows unary operation of autodiff instance.\"\"\"\n",
    "        anew = autodiff(self.name, -self.val, self.der)\n",
    "        anew.lparent = self\n",
    "        for key in self.der:\n",
    "            anew.der[key] = -1*self.der[key]\n",
    "        self.back_partial_der = -1\n",
    "        anew.function = self.__neg__\n",
    "        return anew\n",
    "\n",
    "\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        \"\"\"Allows multiplication of another autodiff instance, or multiplication of a constant (integer or float).\"\"\"\n",
    "        if isinstance(other, pd.DataFrame):\n",
    "            other = np.asarray(other)\n",
    "        if isinstance(other, (int, float, autodiff, list, np.ndarray)) == False:\n",
    "            raise ValueError(\"Error: Only integer, float, list, numpy arrays, or autodiff instances can be multiplied.\")\n",
    "\n",
    "        if isinstance(other, list):\n",
    "            other = np.asarray(other)\n",
    "\n",
    "\n",
    "        anew = autodiff(self.name, self.val, self.der)\n",
    "\n",
    "        #Stores for backpropagation functionality\n",
    "        anew.lparent = self\n",
    "        anew.rparent = other\n",
    "\n",
    "        anew.function=self.__mul__\n",
    "\n",
    "        #for data/gradient descent\n",
    "        #if \n",
    "\n",
    "        #assuming that other is autodiff instance\n",
    "        try:\n",
    "            anew.val = self.val*other.val\n",
    "            for key in np.unique([key for key in self.der] + [key for key in other.der]):\n",
    "                if key not in self.der:\n",
    "                    anew.der[key]=self.val*(other.der[key])\n",
    "                elif key not in other.der:\n",
    "                    anew.der[key]=(self.der[key])*other.val\n",
    "                else:\n",
    "                    anew.der[key]=(self.der[key])*other.val+(other.der[key])*self.val\n",
    "\n",
    "            #set the back partial derivatives that can be used for backpropagation\n",
    "            self.back_partial_der = other.val\n",
    "            other.back_partial_der = self.val\n",
    "\n",
    "\n",
    "        # if 'other' is not autodiff instance\n",
    "        except AttributeError:\n",
    "            # assuming that 'other' is a valid constant\n",
    "            if isinstance(other, (int,float)):\n",
    "                anew.val = self.val*other\n",
    "                for key in self.der:\n",
    "                    anew.der[key] = other*self.der[key]\n",
    "                self.back_partial_der = other\n",
    "\n",
    "            else:\n",
    "                other = np.asarray(other)\n",
    "                if other.shape!=anew.val.shape:\n",
    "                    anew.val = np.dot(other,self.val)\n",
    "                else:\n",
    "                    anew.val = self.val*other\n",
    "\n",
    "                for key in self.der:\n",
    "                    anew.der[key] = np.dot(other,self.der[key])\n",
    "                try:\n",
    "                    fder = [[] for i in range(len(other[0]))]\n",
    "                    for idx,value in enumerate(other):\n",
    "                        for idx2,value2 in enumerate(value):\n",
    "                            fder[idx2].append(value2)\n",
    "                except:\n",
    "                    \n",
    "                    fder = other\n",
    "\n",
    "                self.back_partial_der = fder\n",
    "\n",
    "        return anew\n",
    "\n",
    "    __rmul__ = __mul__\n",
    "\n",
    "    def __truediv__(self,other):\n",
    "        '''function for left division'''\n",
    "\n",
    "        if isinstance(other, (int, float, list, np.ndarray, autodiff)) == False:\n",
    "            raise ValueError(\"Error: Only integer, float, list, numpy arrays, or autodiff instances can be divided.\")\n",
    "        if isinstance(other,list):\n",
    "            other = np.asarray(other)\n",
    "        anew = autodiff(self.name, self.val, self.der)\n",
    "        anew.lparent = self\n",
    "        anew.rparent = other\n",
    "\n",
    "        anew.function = self.__truediv__\n",
    "        self.back_partial_der = 1/other\n",
    "        try:\n",
    "            anew.val = self.val/other.val\n",
    "            self.back_partial_der = 1/other\n",
    "\n",
    "            for key in np.unique([key for key in self.der] + [key for key in other.der]):\n",
    "                if key not in self.der:\n",
    "                    anew.der[key]=self.val*other.der[key]/other.val**2\n",
    "                elif key not in other.der:\n",
    "                    anew.der[key]=self.der[key]/other.val\n",
    "                else:\n",
    "                    anew.der[key]=0\n",
    "        except AttributeError:\n",
    "            anew.val = self.val/other\n",
    "            for key in self.der:\n",
    "                anew.der[key] = (self.der[key])/other\n",
    "                \n",
    "                self.back_partial_der = 1/other\n",
    "\n",
    "        \n",
    "\n",
    "        return anew\n",
    "\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        '''function for right division'''\n",
    "\n",
    "        if isinstance(other, (int, float, list, np.ndarray, autodiff)) == False:\n",
    "            raise ValueError(\"Error: Only integer, float, list, numpy arrays, or autodiff instances can be divided.\")\n",
    "        if isinstance(other,list):\n",
    "            other = np.asarray(other)\n",
    "\n",
    "        anew = autodiff(self.name, self.val, self.der)\n",
    "        anew.lparent = self\n",
    "        anew.rparent = other\n",
    "\n",
    "        anew.function=self.__rtruediv__\n",
    "        if isinstance(other, (int,float,list,np.ndarray)):\n",
    "            for key in self.der:\n",
    "                anew.der[key] = -other*(self.der[key])/self.val**2\n",
    "                anew.val = other/self.val\n",
    "                self.back_partial_der = -1*(self.val**2)\n",
    "            return anew\n",
    "        else:\n",
    "            raise ValueError(\"Error: Only integer, float, list, numpy arrays, or autodiff instances can be divided.\")\n",
    "\n",
    "\n",
    "\n",
    "    def __add__(self, other):\n",
    "\n",
    "        if isinstance(other, (int, float, autodiff, list, np.ndarray)) == False:\n",
    "            raise ValueError(\"Error: Only integer, float, list, numpy arrays, or autodiff instances can be added.\")\n",
    "\n",
    "\n",
    "        #Generate a new autodiff instance copy of self\n",
    "        anew = autodiff(self.name, self.val, self.der)\n",
    "\n",
    "        anew.function=self.__add__\n",
    "\n",
    "        anew.lparent = self\n",
    "        anew.rparent = other\n",
    "\n",
    "\n",
    "        #Tries adding two autodiff instances together\n",
    "        try:\n",
    "\n",
    "            #Add values\n",
    "            anew.val = self.val + other.val\n",
    "\n",
    "            #Calculate derivatives of this addition for all variables so far encountered\n",
    "            for key in np.unique([key for key in self.der] + [key for key in other.der]): #Iterate through all unique variables so far encountered\n",
    "                #If self has not encountered this variable before (so derivative of self with respect to variable must be 0)\n",
    "                if key not in self.der:\n",
    "                    anew.der[key] = other.der[key]\n",
    "\n",
    "                #Else, if opponent has not encountered this variable before (so derivative of self with respect to variable must be 0)\n",
    "                elif key not in other.der:\n",
    "                    anew.der[key] = self.der[key]\n",
    "\n",
    "                #Else, if both self and opponent have encountered this variable before\n",
    "                else:\n",
    "                    anew.der[key] = self.der[key] + other.der[key]\n",
    "\n",
    "            self.back_partial_der = 1\n",
    "            other.back_partial_der = 1\n",
    "\n",
    "        #Otherwise, if not two autodiff instances:\n",
    "        except AttributeError:\n",
    "            #Tries adding autodiff instance and number together\n",
    "\n",
    "            for key in self.der:\n",
    "                anew.der[key] = self.der[key]\n",
    "                anew.val = other + self.val\n",
    "            self.back_partial_der = 1\n",
    "\n",
    "        #Returns new autodiff instance\n",
    "        return anew\n",
    "\n",
    "\n",
    "\n",
    "    #FUNCTION: __radd__\n",
    "    #PURPOSE: Allows commutative addition.\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    #FUNCTION: __sub__\n",
    "    #PURPOSE: Subtract an autodiff instance or number from a autodiff instance, and calculate the derivatives resulting from this action.\n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, (int, float, autodiff, list, np.ndarray)) == False:\n",
    "            raise ValueError(\"Error: Only integer, float, list, numpy arrays, or autodiff instances can be subtracted.\")\n",
    "\n",
    "        #Generate a new autodiff instance copy of self\n",
    "        anew = autodiff(self.name, self.val, self.der)\n",
    "        anew.function=self.__sub__\n",
    "\n",
    "        anew.lparent = self\n",
    "        anew.rparent = other\n",
    "\n",
    "        #Tries subtracting two autodiff instances together\n",
    "        try:\n",
    "            #Subtract values\n",
    "            anew.val = self.val - other.val\n",
    "\n",
    "            #Calculate derivatives of this subtraction for all variables so far encountered\n",
    "            for key in np.unique([key for key in self.der] + [key for key in other.der]): #Iterate through all unique variables so far encountered\n",
    "                #If self has not encountered this variable before (so derivative of self with respect to variable must be 0)\n",
    "                if key not in self.der:\n",
    "                    anew.der[key] = -1*other.der[key]\n",
    "\n",
    "                #Else, if opponent has not encountered this variable before (so derivative of self with respect to variable must be 0)\n",
    "                elif key not in other.der:\n",
    "                    anew.der[key] = self.der[key]\n",
    "\n",
    "                #Else, if both self and opponent have encountered this variable before\n",
    "                else:\n",
    "                    anew.der[key] = self.der[key] - other.der[key]\n",
    "\n",
    "        #Otherwise, if not two autodiff instances:\n",
    "        except AttributeError:\n",
    "            #Tries subtracting number from autodiff instance\n",
    "\n",
    "            for key in self.der:\n",
    "                anew.der[key] = self.der[key]\n",
    "                anew.val = self.val - other\n",
    "\n",
    "        #Returns new autodiff instance\n",
    "        self.back_partial_der = 1\n",
    "        return anew\n",
    "\n",
    "\n",
    "\n",
    "    #FUNCTION: __sub__\n",
    "    #PURPOSE: Subtract an autodiff instance or number from a autodiff instance, and calculate the derivatives resulting from this action.\n",
    "    def __rsub__(self, other):\n",
    "        return (-1*self) + other\n",
    "\n",
    "\n",
    "    #FUNCTION: __pow__\n",
    "    #PURPOSE: Raise this autodiff instance to a number or to another autodiff instance, and calculate the derivatives resulting from this action.\n",
    "    def __pow__(self, other):\n",
    "\n",
    "        if isinstance(other, (int, float, autodiff, list, np.ndarray)) == False:\n",
    "            raise ValueError(\"Error: Only integer, float, or autodiff instances can be .\")\n",
    "\n",
    "        #Generate a new autodiff instance copy of self\n",
    "        anew = autodiff(self.name, self.val, self.der)\n",
    "        anew.function=self.__pow__\n",
    "\n",
    "        anew.lparent = self\n",
    "        anew.rparent = other\n",
    "\n",
    "        #Tries raising this autodiff instance to another autodiff instance\n",
    "        try:\n",
    "            #Raise values\n",
    "            anew.val = self.val**other.val\n",
    "\n",
    "            #Calculate derivatives of this exponentiation for all variables so far encountered\n",
    "            for key in np.unique([key for key in self.der] + [key for key in other.der]): #Iterate through all unique variables so far encountered\n",
    "                #If self has not encountered this variable before (so derivative of self with respect to variable must be 0)\n",
    "                if key not in self.der:\n",
    "                    anew.der[key] = anew.val*(np.log(self.val)*other.der[key])\n",
    "\n",
    "                #Else, if opponent has not encountered this variable before (so derivative of self with respect to variable must be 0)\n",
    "                elif key not in other.der:\n",
    "                    anew.der[key] = anew.val*(other.val*self.der[key]/1.0/self.val)\n",
    "\n",
    "                #Else, if both self and opponent have encountered this variable before\n",
    "                else:\n",
    "                    anew.der[key] = anew.val*((np.log(self.val)*other.der[key]) + (other.val*self.der[key]/1.0/self.val))\n",
    "\n",
    "            self.back_partial_der = other.val*self.val**(other.val-1)\n",
    "            other.back_partial_der = (self.val**other.val)*np.log(self.val)\n",
    "\n",
    "        #Otherwise, if not two autodiff instances:\n",
    "        except AttributeError:\n",
    "            #Tries adding autodiff instance and number together\n",
    "            for key in self.der:\n",
    "                anew.der[key] = other*(self.val**(other - 1))*self.der[key]\n",
    "                anew.val = self.val**other\n",
    "            self.back_partial_der = other*self.val**(other-1)\n",
    "        #Returns new autodiff instance\n",
    "        return anew\n",
    "\n",
    "\n",
    "    #FUNCTION: __rpow__\n",
    "    #PURPOSE: Raise this number to an autodiff instance, and calculate the derivatives resulting from this action.\n",
    "    def __rpow__(self, other):\n",
    "        if isinstance(other, (int, float, autodiff)) == False:\n",
    "            raise ValueError(\"Error: Only integer, float, or autodiff instances can be multiplied.\")\n",
    "\n",
    "        #Generate a new autodiff instance copy of self\n",
    "        anew = autodiff(self.name, self.val, self.der)\n",
    "        anew.function=self.__rpow__\n",
    "        #Tries autodiff instance and number together\n",
    "        for key in self.der:\n",
    "            anew.der[key] = (other**self.val)*np.log(other)*self.der[key]\n",
    "            anew.val = other**self.val\n",
    "        self.back_partial_der = other**(self.val)*np.log(other)\n",
    "        #Return new autodiff instance\n",
    "        return anew\n",
    "\n",
    "    def jacobian(self, order=None):\n",
    "        if order is not None: # If specific ordering requested\n",
    "            order = list(order)\n",
    "            jacobian = [None]*len(order)\n",
    "            ii = 0 # For indexing through jacobian\n",
    "            try:\n",
    "                for key in order:\n",
    "                    jacobian[ii] = self.der[key]\n",
    "                    ii = ii + 1\n",
    "            except KeyError:\n",
    "                raise KeyError(\"Error: variable(s) in order have not been encountered by this autodiff instance.\")\n",
    "        \n",
    "        else: # If no specific ordering given\n",
    "            jacobian = [None]*len(self.der)\n",
    "            order = [None]*len(self.der) # To hold ordering\n",
    "            ii = 0 # For indexing through jacobian\n",
    "            for key in self.der:\n",
    "                order[ii] = key\n",
    "                jacobian[ii] = self.der[key]\n",
    "                ii = ii + 1\n",
    "       \n",
    "        # Cast the output as an array\n",
    "        jacobian = np.asarray(jacobian)\n",
    "        # Return jacobian and its ordering\n",
    "        return {\"jacobian\":jacobian, \"order\":order}\n",
    "\n",
    "\n",
    "    def backprop(self,y_true, loss = 'MSE', backproplist = None, loss_value = 0):\n",
    "        if backproplist == None:\n",
    "            if isinstance(y_true,list):\n",
    "                y_true = np.asarray(y_true)\n",
    "            elif not isinstance(y_true, np.ndarray):\n",
    "                y_true = np.asarray([y_true])\n",
    "            backproplist = {}\n",
    "            \n",
    "            if loss == 'MSE':\n",
    "                d_loss = (2/y_true.shape[0]*(self.val-y_true))\n",
    "                loss_value = (1/y_true.shape[0])*np.sum((self.val-y_true)**2)\n",
    "            elif loss == 'MAE':\n",
    "                d_loss = []\n",
    "                for idx, yt in enumerate(y_true):\n",
    "                    if self.val[idx]-yt>=0:\n",
    "                        d_loss.append(1/y_true.shape[0])\n",
    "                    else:\n",
    "                        d_loss.append(-1/y_true.shape[0])\n",
    "                d_loss = np.asarray(d_loss)\n",
    "                loss_value = (1/y_true.shape[0])*np.sum(np.absolute((self.val-y_true)))\n",
    "            elif loss == 'RMSE':\n",
    "                d_loss = (1/y_true.shape[0])**(-0.5)*(self.val-y_true)/(np.sum((self.val-y_true)**2))\n",
    "                loss_value = ((1/y_true.shape[0])**(0.5)*np.sum((self.val-y_true)**2))**(0.5)\n",
    "            elif loss  == 'MPAE':\n",
    "                d_loss = []\n",
    "                for idx, yt in enumerate(y_true):\n",
    "                    if self.val[idx]-yt>=0:\n",
    "                        d_loss.append(((self.val[idx]-yt)/yt)/y_true.shape[0])\n",
    "                    else:\n",
    "                        d_loss.append(-1*((self.val[idx]-yt)/yt)/y_true.shape[0])\n",
    "                d_loss = np.asarray(d_loss)\n",
    "                loss_value = (1/y_true.shape[0])*np.sum(np.absolute((self.val-y_true)))\n",
    "            elif loss == 'cross entropy':\n",
    "                if max(y_true) != 1 and min(y_true) != 0:\n",
    "                    raise ValueError('Values must be binary (0 or 1)')\n",
    "                d_loss = -(y_true/self.val + (y_true-1)/(1-self.val))\n",
    "                loss_value = -y_true*np.log(self.val)-self.val*np.log(y_true)\n",
    "\n",
    "            self.back_der = d_loss\n",
    "\n",
    "        if self.lparent:\n",
    "            try:\n",
    "                self.lparent.back_der = self.back_der*self.lparent.back_partial_der\n",
    "                self.lparent.backprop(y_true,loss,backproplist, loss_value)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "        if isinstance(self.rparent, autodiff):\n",
    "            try:\n",
    "                self.rparent.back_der = self.back_der*self.rparent.back_partial_der\n",
    "                self.rparent.backprop(y_true,loss,backproplist,loss_value)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "        if self.lparent is None and self.rparent is None:\n",
    "            backproplist[self.name] = self.back_der\n",
    "        \n",
    "\n",
    "        return (backproplist, loss_value)\n",
    "\n",
    "\n",
    "    def forwardprop(self):\n",
    "        if self.lparent.lparent is None:\n",
    "            self.lparent.forwardpropcomplete = \"Yes\"\n",
    "        if isinstance(self.rparent, autodiff) and self.rparent.lparent is None:\n",
    "            self.rparent.forwardpropcomplete = \"Yes\"\n",
    "\n",
    "        \n",
    "        if self.lparent.forwardpropcomplete == 'No':\n",
    "            self.lparent=self.lparent.forwardprop()\n",
    "        \n",
    "        if isinstance(self.rparent,autodiff):\n",
    "            if self.rparent.forwardpropcomplete == 'Yes':\n",
    "                self = self.function(self.rparent)\n",
    "            else:\n",
    "                self.rparent=self.rparent.forwardprop()\n",
    "        else:\n",
    "            if 'mul' in str(self.function):\n",
    "                self.function = self.lparent.__mul__\n",
    "            elif 'add' in str(self.function):\n",
    "                self.function = self.lparent.__add__\n",
    "            elif 'sub' in str(self.function):\n",
    "                self.function = self.lparent.__sub__\n",
    "            elif 'div' in str(self.function):\n",
    "                self.function = self.lparent.__truediv__\n",
    "            elif 'pow' in str(self.function):\n",
    "                self.function = self.lparent.__pow__\n",
    "            elif 'neg' in str(self.function):\n",
    "                self.function = self.lparent.__neg__\n",
    "            if (isinstance(self.rparent, (int,float,list,np.ndarray,autodiff))):\n",
    "                self = self.function(self.rparent)\n",
    "            else:\n",
    "                self = self.function(self.lparent)\n",
    "\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def weight_update(self,delta,learning_rate):\n",
    "        a = []\n",
    "        for idx,value in enumerate(delta):\n",
    "            a.append(np.sum(value))\n",
    "        delta = a\n",
    "        learning_rate = np.asarray(learning_rate)\n",
    "        self.val = self.val-learning_rate*delta\n",
    "\n",
    "\n",
    "\n",
    "def gradient_descent(f,y_true, loss = 'MSE', beta = 0.01, max_iter = 10000):\n",
    "    #get w from f\n",
    "    j = 0\n",
    "    w=f.lparent\n",
    "    while j<100 and w.lparent is not None:\n",
    "        w = w.lparent\n",
    "    if w.name != 'w':\n",
    "        raise ValueError('Could not find weight vector. Be sure to name the weight autodiff as \"w\"')\n",
    "\n",
    "    loss_values = []\n",
    "    \n",
    "    i = 0\n",
    "    loss_v = 1\n",
    "    while i<max_iter and loss_v>10**(-8):\n",
    "        backprop_ans = f.backprop(y_true, loss = loss)\n",
    "        delta = backprop_ans[0]\n",
    "        loss_v = backprop_ans[1]\n",
    "        loss_values.append(loss_v)\n",
    "        l = beta*w.val/np.sum(np.absolute(delta['w']), axis=1)\n",
    "        w.weight_update(delta['w'],l)\n",
    "        f = f.forwardprop()\n",
    "        i=i+1\n",
    "\n",
    "    return f,w,loss_values,i\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.6.6, pytest-3.8.2, py-1.6.0, pluggy-0.7.1\n",
      "rootdir: C:\\Users\\wyd15\\Desktop\\Course\\2018Fall, inifile:\n",
      "plugins: remotedata-0.3.0, openfiles-0.3.0, doctestplus-0.1.3, cov-2.6.0, arraydiff-0.2\n",
      "collected 68 items / 2 errors\n",
      "\n",
      "=================================== ERRORS ====================================\n",
      "__________ ERROR collecting cs207_yudi_wang/lectures/L11/test_L2.py ___________\n",
      "import file mismatch:\n",
      "imported module 'test_L2' has this __file__ attribute:\n",
      "  C:\\Users\\wyd15\\Desktop\\Course\\2018Fall\\cs207-F18\\lectures\\L11\\test_L2.py\n",
      "which is not the same as the test file we want to collect:\n",
      "  C:\\Users\\wyd15\\Desktop\\Course\\2018Fall\\cs207_yudi_wang\\lectures\\L11\\test_L2.py\n",
      "HINT: remove __pycache__ / .pyc files and/or use a unique basename for your test file modules\n",
      "__________________ ERROR collecting cs207test/test_roots.py ___________________\n",
      "import file mismatch:\n",
      "imported module 'test_roots' has this __file__ attribute:\n",
      "  C:\\Users\\wyd15\\Desktop\\Course\\2018Fall\\cs207_yudi_wang\\homework\\HW4\\test_roots.py\n",
      "which is not the same as the test file we want to collect:\n",
      "  C:\\Users\\wyd15\\Desktop\\Course\\2018Fall\\cs207test\\test_roots.py\n",
      "HINT: remove __pycache__ / .pyc files and/or use a unique basename for your test file modules\n",
      "!!!!!!!!!!!!!!!!!!! Interrupted: 2 errors during collection !!!!!!!!!!!!!!!!!!!\n",
      "=========================== 2 error in 1.79 seconds ===========================\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"C:\\\\Users\\\\wyd15\\\\Desktop\\\\Course\\\\2018Fall\\\\cs207-FinalProject\")\n",
    "\n",
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: [8 5 5]\n",
      "derivatives:{'w': array([6, 4, 3], dtype=int64), 'y': array([4, 5, 5], dtype=int64)}\n",
      "[8 5 5]\n"
     ]
    }
   ],
   "source": [
    "import autodiff as ad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x = pd.DataFrame([[1,2],[1,3],[2,1]])\n",
    "w = autodiff('w', [2,1])\n",
    "y = autodiff('y', [2,1,1])\n",
    "f=w*x*y\n",
    "g = gradient_descent(f,[1,1,2],loss= 'MSE')\n",
    "print(f)\n",
    "print(g[0].val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wyd15\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel_launcher.py:428: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true param: [1, 2], final obj error: 1.6433962264150968, param: [1.95660377 0.60188679], iteration: 2000, with starting point: [4, 2]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final obj error: 0.023529411764705854, param: [0.96470588 2.05882353], iteration: 2000, with starting point: [1.5, 2.5]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class autodiff():\n",
    "    def __init__(self,name,val,der=1):\n",
    "        self.name = name\n",
    "        # set val attribute\n",
    "        if isinstance(val, np.ndarray):\n",
    "            self.val = val\n",
    "        elif isinstance(val, list):\n",
    "            self.val = np.asarray(val)\n",
    "        else:\n",
    "            self.val = np.asarray([val])\n",
    "\n",
    "        if isinstance(der, np.ndarray):\n",
    "            self.der = {name:der}\n",
    "        elif isinstance(der, list):\n",
    "            self.der = {name:np.asarray(der)}\n",
    "        else:\n",
    "            self.der = {name:np.asarray([der]*self.val.shape[0])}\n",
    "\n",
    "\n",
    "        self.lparent = None\n",
    "        self.rparent = None\n",
    "\n",
    "        self.forwardpropcomplete = 'No'\n",
    "\n",
    "        self.function = None\n",
    "\n",
    "        self.back_der = None\n",
    "        self.back_partial_der = None\n",
    "\n",
    "\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, autodiff) == False:\n",
    "            raise ValueError(\"Error: only autodiff instances can be compared with another.\")\n",
    "        return (self.val == other.val) and (self.der == other.der)\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        return not (self == other)\n",
    "\n",
    "    def __neg__(self):\n",
    "        \"\"\"Allows unary operation of autodiff instance.\"\"\"\n",
    "        anew = autodiff(self.name, -self.val, self.der)\n",
    "        for key in self.der:\n",
    "            anew.der[key] = -self.der[key]\n",
    "        return anew\n",
    "\n",
    "\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        \"\"\"Allows multiplication of another autodiff instance, or multiplication of a constant (integer or float).\"\"\"\n",
    "\n",
    "        if isinstance(other, (int, float, autodiff, list, np.ndarray)) == False:\n",
    "            print(other)\n",
    "            raise ValueError(\"Error: Only integer, float, or autodiff instances can be multiplied.\")\n",
    "\n",
    "        if isinstance(other, list):\n",
    "            other = np.asarray(other)\n",
    "\n",
    "\n",
    "        anew = autodiff(self.name, self.val, self.der)\n",
    "\n",
    "        #Stores for backpropagation functionality\n",
    "        anew.lparent = self\n",
    "        anew.rparent = other\n",
    "\n",
    "        anew.function=anew.__mul__\n",
    "\n",
    "        #assuming that other is autodiff instance\n",
    "        try:\n",
    "            anew.val = self.val*other.val\n",
    "            for key in np.unique([key for key in self.der] + [key for key in other.der]):\n",
    "                if key not in self.der:\n",
    "                    anew.der[key]=self.val*(other.der[key])\n",
    "                elif key not in other.der:\n",
    "                    anew.der[key]=(self.der[key])*other.val\n",
    "                else:\n",
    "                    anew.der[key]=(self.der[key])*other.val+(other.der[key])*self.val\n",
    "\n",
    "            #set the back partial derivatives that can be used for backpropagation\n",
    "            self.back_partial_der = other.val\n",
    "            other.back_partial_der = self.val\n",
    "\n",
    "\n",
    "        # if 'other' is not autodiff instance\n",
    "        except AttributeError:\n",
    "            # assuming that 'other' is a valid constant\n",
    "            for key in self.der:\n",
    "                anew.der[key] = other*self.der[key]\n",
    "                anew.val = other*self.val\n",
    "                self.back_partial_der = other\n",
    "\n",
    "\n",
    "        return anew\n",
    "\n",
    "    __rmul__ = __mul__\n",
    "\n",
    "    def __truediv__(self,other):\n",
    "        '''function for left division'''\n",
    "\n",
    "        if isinstance(other, (int, float, autodiff)) == False:\n",
    "            raise ValueError(\"Error: Only integer, float, or autodiff instances can be divided.\")\n",
    "\n",
    "        anew = autodiff(self.name, self.val, self.der)\n",
    "        anew.function = anew.__truediv__\n",
    "        try:\n",
    "            anew.val = self.val/other.val\n",
    "\n",
    "            for key in np.unique([key for key in self.der] + [key for key in other.der]):\n",
    "                if key not in self.der:\n",
    "                    anew.der[key]=self.val*other.der[key]/other.val**2\n",
    "                elif key not in other.der:\n",
    "                    anew.der[key]=self.der[key]/other.val\n",
    "                else:\n",
    "                    anew.der[key]=0\n",
    "        except AttributeError:\n",
    "            for key in self.der:\n",
    "                anew.der[key] = (self.der[key])/other\n",
    "                anew.val = self.val/other\n",
    "        return anew\n",
    "\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "\n",
    "        '''function for right division, when performing (constant)/(autodiff)'''\n",
    "        anew.function=anew.__rtruediv__\n",
    "        if isinstance(other, (int,float)):\n",
    "            anew = autodiff(self.name, self.val, self.der)\n",
    "            for key in self.der:\n",
    "                anew.der[key] = -other*(self.der[key])/self.val**2\n",
    "                anew.val = other/self.val\n",
    "            return anew\n",
    "        else:\n",
    "            raise ValueError(\"Error: Only integer, float, or autodiff instances can be divided.\")\n",
    "\n",
    "\n",
    "\n",
    "    def __add__(self, other):\n",
    "\n",
    "        if isinstance(other, (int, float, autodiff)) == False:\n",
    "            raise ValueError(\"Error: Only integer, float, or autodiff instances can be added.\")\n",
    "\n",
    "\n",
    "        #Generate a new autodiff instance copy of self\n",
    "        anew = autodiff(self.name, self.val, self.der)\n",
    "\n",
    "        anew.function=anew.__add__\n",
    "\n",
    "        anew.lparent = self\n",
    "        anew.rparent = other\n",
    "\n",
    "\n",
    "        #Tries adding two autodiff instances together\n",
    "        try:\n",
    "\n",
    "            #Add values\n",
    "            anew.val = self.val + other.val\n",
    "\n",
    "            #Calculate derivatives of this addition for all variables so far encountered\n",
    "            for key in np.unique([key for key in self.der] + [key for key in other.der]): #Iterate through all unique variables so far encountered\n",
    "                #If self has not encountered this variable before (so derivative of self with respect to variable must be 0)\n",
    "                if key not in self.der:\n",
    "                    anew.der[key] = other.der[key]\n",
    "\n",
    "                #Else, if opponent has not encountered this variable before (so derivative of self with respect to variable must be 0)\n",
    "                elif key not in other.der:\n",
    "                    anew.der[key] = self.der[key]\n",
    "\n",
    "                #Else, if both self and opponent have encountered this variable before\n",
    "                else:\n",
    "                    anew.der[key] = self.der[key] + other.der[key]\n",
    "\n",
    "            self.back_partial_der = 1\n",
    "            other.back_partial_der = 1\n",
    "\n",
    "        #Otherwise, if not two autodiff instances:\n",
    "        except AttributeError:\n",
    "            #Tries adding autodiff instance and number together\n",
    "\n",
    "            for key in self.der:\n",
    "                anew.der[key] = self.der[key]\n",
    "                anew.val = other + self.val\n",
    "            self.back_partial_der = 1\n",
    "\n",
    "        #Returns new autodiff instance\n",
    "        return anew\n",
    "\n",
    "\n",
    "\n",
    "    #FUNCTION: __radd__\n",
    "    #PURPOSE: Allows commutative addition.\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    #FUNCTION: __sub__\n",
    "    #PURPOSE: Subtract an autodiff instance or number from a autodiff instance, and calculate the derivatives resulting from this action.\n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, (int, float, autodiff)) == False:\n",
    "            raise ValueError(\"Error: Only integer, float, or autodiff instances can be subtracted.\")\n",
    "\n",
    "        #Generate a new autodiff instance copy of self\n",
    "        anew = autodiff(self.name, self.val, self.der)\n",
    "        anew.function=anew.__sub__\n",
    "\n",
    "        #Tries subtracting two autodiff instances together\n",
    "        try:\n",
    "            #Subtract values\n",
    "            anew.val = self.val - other.val\n",
    "\n",
    "            #Calculate derivatives of this subtraction for all variables so far encountered\n",
    "            for key in np.unique([key for key in self.der] + [key for key in other.der]): #Iterate through all unique variables so far encountered\n",
    "                #If self has not encountered this variable before (so derivative of self with respect to variable must be 0)\n",
    "                if key not in self.der:\n",
    "                    anew.der[key] = -1*other.der[key]\n",
    "\n",
    "                #Else, if opponent has not encountered this variable before (so derivative of self with respect to variable must be 0)\n",
    "                elif key not in other.der:\n",
    "                    anew.der[key] = self.der[key]\n",
    "\n",
    "                #Else, if both self and opponent have encountered this variable before\n",
    "                else:\n",
    "                    anew.der[key] = self.der[key] - other.der[key]\n",
    "\n",
    "        #Otherwise, if not two autodiff instances:\n",
    "        except AttributeError:\n",
    "            #Tries subtracting number from autodiff instance\n",
    "\n",
    "            for key in self.der:\n",
    "                anew.der[key] = self.der[key]\n",
    "                anew.val = self.val - other\n",
    "\n",
    "        #Returns new autodiff instance\n",
    "        return anew\n",
    "\n",
    "\n",
    "\n",
    "    #FUNCTION: __sub__\n",
    "    #PURPOSE: Subtract an autodiff instance or number from a autodiff instance, and calculate the derivatives resulting from this action.\n",
    "    def __rsub__(self, other):\n",
    "        return (-1*self) + other\n",
    "\n",
    "\n",
    "    #FUNCTION: __pow__\n",
    "    #PURPOSE: Raise this autodiff instance to a number or to another autodiff instance, and calculate the derivatives resulting from this action.\n",
    "    def __pow__(self, other):\n",
    "\n",
    "        if isinstance(other, (int, float, autodiff)) == False:\n",
    "            raise ValueError(\"Error: Only integer, float, or autodiff instances can be multiplied.\")\n",
    "\n",
    "        #Generate a new autodiff instance copy of self\n",
    "        anew = autodiff(self.name, self.val, self.der)\n",
    "        anew.function=anew.__pow__\n",
    "\n",
    "        anew.lparent = self\n",
    "        anew.rparent = other\n",
    "\n",
    "        #Tries raising this autodiff instance to another autodiff instance\n",
    "        try:\n",
    "            #Raise values\n",
    "            anew.val = self.val**other.val\n",
    "\n",
    "            #Calculate derivatives of this exponentiation for all variables so far encountered\n",
    "            for key in np.unique([key for key in self.der] + [key for key in other.der]): #Iterate through all unique variables so far encountered\n",
    "                #If self has not encountered this variable before (so derivative of self with respect to variable must be 0)\n",
    "                if key not in self.der:\n",
    "                    anew.der[key] = anew.val*(np.log(self.val)*other.der[key])\n",
    "\n",
    "                #Else, if opponent has not encountered this variable before (so derivative of self with respect to variable must be 0)\n",
    "                elif key not in other.der:\n",
    "                    anew.der[key] = anew.val*(other.val*self.der[key]/1.0/self.val)\n",
    "\n",
    "                #Else, if both self and opponent have encountered this variable before\n",
    "                else:\n",
    "                    anew.der[key] = anew.val*((np.log(self.val)*other.der[key]) + (other.val*self.der[key]/1.0/self.val))\n",
    "\n",
    "            self.back_partial_der = other.val*self.val**(other.val-1)\n",
    "            other.back_partial_der = (self.val**other.val)*np.log(self.val)\n",
    "\n",
    "        #Otherwise, if not two autodiff instances:\n",
    "        except AttributeError:\n",
    "            #Tries adding autodiff instance and number together\n",
    "            for key in self.der:\n",
    "                anew.der[key] = other*(self.val**(other - 1))*self.der[key]\n",
    "                anew.val = self.val**other\n",
    "            self.back_partial_der = other*self.val**(other-1)\n",
    "        #Returns new autodiff instance\n",
    "        return anew\n",
    "\n",
    "\n",
    "    #FUNCTION: __rpow__\n",
    "    #PURPOSE: Raise this number to an autodiff instance, and calculate the derivatives resulting from this action.\n",
    "    def __rpow__(self, other):\n",
    "        if isinstance(other, (int, float, autodiff)) == False:\n",
    "            raise ValueError(\"Error: Only integer, float, or autodiff instances can be multiplied.\")\n",
    "\n",
    "        #Generate a new autodiff instance copy of self\n",
    "        anew = autodiff(self.name, self.val, self.der)\n",
    "        anew.function=anew.__rpow__\n",
    "        #Tries autodiff instance and number together\n",
    "        for key in self.der:\n",
    "            anew.der[key] = (other**self.val)*np.log(other)*self.der[key]\n",
    "            anew.val = other**self.val\n",
    "\n",
    "        #Return new autodiff instance\n",
    "        return anew\n",
    "\n",
    "    def jacobian(self, order=None):\n",
    "        if order is not None: # If specific ordering requested\n",
    "            order = list(order)\n",
    "            jacobian = [None]*len(order)\n",
    "            ii = 0 # For indexing through jacobian\n",
    "            try:\n",
    "                for key in order:\n",
    "                    jacobian[ii] = self.der[key]\n",
    "                    ii = ii + 1\n",
    "            except KeyError:\n",
    "                raise KeyError(\"Error: variable(s) in order have not been encountered by this autodiff instance.\")\n",
    "        \n",
    "        else: # If no specific ordering given\n",
    "            jacobian = [None]*len(self.der)\n",
    "            order = [None]*len(self.der) # To hold ordering\n",
    "            ii = 0 # For indexing through jacobian\n",
    "            for key in self.der:\n",
    "                order[ii] = key\n",
    "                jacobian[ii] = self.der[key]\n",
    "                ii = ii + 1\n",
    "       \n",
    "        # Cast the output as an array\n",
    "        jacobian = np.asarray(jacobian)\n",
    "        # Return jacobian and its ordering\n",
    "        return {\"jacobian\":jacobian, \"order\":order}\n",
    "\n",
    "\n",
    "    def backprop(self,y_true, loss = 'MSE', backproplist = None):\n",
    "\n",
    "\n",
    "        if backproplist == None:\n",
    "            if isinstance(y_true,list):\n",
    "                y_true = np.asarray(y_true)\n",
    "            elif not isinstance(y_true, np.ndarray):\n",
    "                y_true = np.nsarray([y_true])\n",
    "            backproplist = {}\n",
    "            \n",
    "            if loss == 'MSE':\n",
    "                #d_loss = (2/y_true.shape[0]*(self.val-y_true))#np.average/mean\n",
    "                d_loss = np.mean((2/y_true.shape[0]*(self.val-y_true)))#np.average/mean\n",
    "                #print(d_loss)\n",
    "            elif loss == 'MAE':\n",
    "                d_loss = []\n",
    "                for idx, yt in enumerate(y_true):\n",
    "                    if self.val[idx]-yt>=0:\n",
    "                        d_loss.append(1/y_true.shape[0])\n",
    "                    else:\n",
    "                        d_loss.append(-1/y_true.shape[0])\n",
    "            elif loss == 'RMSE':\n",
    "                d_loss = y_true.shape[0]**(-0.5)*(self.val-y_true)**0.5\n",
    "            elif loss  == 'MPAE':\n",
    "                d_loss = []\n",
    "                for idx, yt in enumerate(y_true):\n",
    "                    if self.val[idx]-yt>=0:\n",
    "                        d_loss.append(((self.val[idx]-yt)/yt)/y_true.shape[0])\n",
    "                    else:\n",
    "                        d_loss.append(-1*((self.val[idx]-yt)/yt)/y_true.shape[0])\n",
    "            elif loss == 'cross entropy':\n",
    "                if max(y_true) != 1 and min(y_true) != 0:\n",
    "                    raise ValueError('Values must be binary (0 or 1)')\n",
    "                d_loss = -(y_true/self.val + (y_true-1)/(1-self.val))\n",
    "\n",
    "\n",
    "            self.back_der = d_loss\n",
    "            #print(self.back_der)\n",
    "\n",
    "\n",
    "        if self.lparent:\n",
    "\n",
    "            try:\n",
    "                self.lparent.back_der = self.back_der*self.lparent.back_partial_der\n",
    "                self.lparent.backprop(y_true,loss,backproplist)\n",
    "            except:\n",
    "                pass\n",
    "        if self.rparent:\n",
    "            try:\n",
    "                self.rparent.back_der = self.back_der*self.rparent.back_partial_der\n",
    "                self.rparent.backprop(y_true,loss,backproplist)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        if self.lparent is None and self.rparent is None:\n",
    "            backproplist[self.name] = self.back_der\n",
    "\n",
    "        return backproplist\n",
    "\n",
    "\n",
    "    def forwardprop(self):\n",
    "        if self.lparent is None and self.rparent is None:\n",
    "            self.forwardpropcomplete = 'Yes'\n",
    "        if self.lparent.forwardpropcomplete == 'Yes':\n",
    "            if isinstance(self.rparent):\n",
    "                if self.rparent.forwardpropcomplete == 'Yes':\n",
    "                    self = self.lparent.function(rparent)\n",
    "                else:\n",
    "                    self.rparent.forwardprop()\n",
    "            else:\n",
    "                self = self.lparent.function(rparent)\n",
    "\n",
    "        return self.der\n",
    "\n",
    "def gradient_descent(func, data, param, loss = 'MSE', N= 2000, tolerance = 1e-9):\n",
    "    iteration = 0\n",
    "    backprop, ypred = func(param, data) \n",
    "    \n",
    "    error = [1]\n",
    "    step = 0.01\n",
    "    path = []\n",
    "    delta_param = np.ones(param.shape)\n",
    "    delta_backprop = np.ones(param.shape)\n",
    "    old_backprop = np.ones(param.shape)\n",
    "    \n",
    "    while iteration<N and error[-1]>tolerance:\n",
    "        #calculate gradient descent\n",
    "        path.append(list(param))\n",
    "        step = max(0.001, np.abs(np.dot(delta_param, delta_backprop))/(np.dot(delta_backprop,delta_backprop)))\n",
    "        param = param - step*backprop\n",
    "        #print(old_backprop)\n",
    "        delta_param = step*backprop\n",
    "        #print(delta_param, delta_backprop)\n",
    "        backprop, ypred = func(param, data)\n",
    "        delta_backprop = backprop-old_backprop\n",
    "        old_backprop = backprop\n",
    "        \n",
    "        error.append(np.sum(abs(ypred - data[-1]))/len(data[-1]))\n",
    "        iteration+=1\n",
    "        \n",
    "    return param, error, np.array(path), iteration\n",
    "\n",
    "\n",
    "def func_lm(param, data):\n",
    "    '''obj function'''\n",
    "    w0 = autodiff('w0', param[0])\n",
    "    X = data[0]\n",
    "    w1 = autodiff('w1', param[1])\n",
    "    Y = data[1]\n",
    "\n",
    "    f = w0*X+w1*Y\n",
    "    backprop = f.backprop(data[2])\n",
    "    ypred= f.val\n",
    "    backprop = list(backprop.values())\n",
    "    backprop = np.array([backprop[0], backprop[1]])\n",
    "    backprop= np.mean(backprop.T, axis= 0)\n",
    "    \n",
    "    return backprop, ypred\n",
    "\n",
    "####demo###\n",
    "# x = autodiff('w0', 2)\n",
    "# X = [1,2,3,4]\n",
    "# y = autodiff('w1',3)\n",
    "# Y = [0,1,2,3]\n",
    "\n",
    "# f = x*X+y*Y\n",
    "# print(f.val)\n",
    "\n",
    "# z=f.forwardprop()\n",
    "# print(z)\n",
    "# print(f.backprop([1, 8, 13, 18]))\n",
    "# print(f.name)\n",
    "param, res, path, iteration = gradient_descent(func_lm,[[1,2,3,4, 5, 4], [0,1,2,3, 1, 6], [1, 4, 7, 10, 7, 16]], np.array([4, 2]), loss = 'MSE')\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "print('true param: [1, 2], final obj error: {0}, param: {1}, iteration: {2}, with starting point: [4, 2]'.format(res[-1], param, iteration))\n",
    "plt.title('residual plot')\n",
    "plt.plot(path[:, 0], label = 'w0')\n",
    "plt.plot(path[:, 1], label = 'w1')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "param, res, path, iteration = gradient_descent(func_lm,[[1,2,3,4], [0,1,2,3], [1, 4, 7, 10]], np.array([1.2, 2.2]), loss = 'MSE')\n",
    "\n",
    "print('final obj error: {0}, param: {1}, iteration: {2}, with starting point: [1.5, 2.5]'.format(res[-1], param, iteration))\n",
    "plt.plot(res, label ='residual')\n",
    "plt.title('residual plot')\n",
    "plt.plot(path[:, 0], label = 'w0')\n",
    "plt.plot(path[:, 1], label = 'w1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backpropagation evaluation result: {'w0': 1, 'w1': array([3]), 'x1': array([1]), 'w2': array([2]), 'x2': array([-2])}\n",
      "auto differentiate function evaluation result: {'w0': 1, 'w1': array([3]), 'w2': array([2]), 'x1': array([1]), 'x2': array([-2])}\n",
      "\n",
      " backpropagation evaluation result: {'w0': array([-0.36787944]), 'w1': array([-1.10363832]), 'x1': array([-0.36787944]), 'w2': array([-0.73575888]), 'x2': array([0.73575888])}\n",
      "auto differentiate function evaluation result: {'w0': array([-0.36787944]), 'w1': array([-1.10363832]), 'w2': array([-0.73575888]), 'x1': array([-0.36787944]), 'x2': array([0.73575888])}\n",
      "\n",
      " backpropagation evaluation result: {'w0': array([0.19661193]), 'w1': array([0.5898358]), 'x1': array([0.19661193]), 'w2': array([0.39322387]), 'x2': array([-0.39322387])}\n",
      "auto differentiate function evaluation result: {'w0': array([0.19661193]), 'w1': array([0.5898358]), 'w2': array([0.39322387]), 'x1': array([0.19661193]), 'x2': array([-0.39322387])}\n"
     ]
    }
   ],
   "source": [
    "x1 = ad.autodiff(name=\"x1\", val=3, der=1)\n",
    "x2 = ad.autodiff(name=\"x2\", val=2, der=1)\n",
    "w0 = ad.autodiff(name=\"w0\", val=2, der=1)\n",
    "w1 = ad.autodiff(name=\"w1\", val=1, der=1)\n",
    "w2 = ad.autodiff(name=\"w2\", val=-2, der=1)\n",
    "\n",
    "f =(w0+w1*x1+w2*x2)\n",
    "print('backpropagation evaluation result:', f.backprop())\n",
    "print('auto differentiate function evaluation result:', f.der)\n",
    "\n",
    "f = admath.exp((-1)*(w0+w1*x1+w2*x2))\n",
    "print('\\n backpropagation evaluation result:', f.backprop())\n",
    "print('auto differentiate function evaluation result:', f.der)\n",
    "\n",
    "f = (1+admath.exp((-1)*(w0+w1*x1+w2*x2)))**(-1)\n",
    "print('\\n backpropagation evaluation result:', f.backprop())\n",
    "print('auto differentiate function evaluation result:', f.der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " backpropagation evaluation result: {'w0': array([1.43239449]), 'w1': array([0.88550171]), 'x1': array([-0.84559184]), 'w2': array([-0.44275085]), 'x2': array([-0.21139796])}\n",
      "auto differentiate function evaluation result: {'w0': array([1.14591559]), 'w1': array([0.88550171]), 'w2': array([-0.44275085]), 'x1': array([-0.97640535]), 'x2': array([0.24410134])}\n"
     ]
    }
   ],
   "source": [
    "#implementing self-test including: truediv, arcsin, arctan, arccos, subtraction etc\n",
    "x1 = ad.autodiff(name=\"x1\", val=0.5, der=1)\n",
    "x2 = ad.autodiff(name=\"x2\", val=0.5, der=1)\n",
    "w0 = ad.autodiff(name=\"w0\", val=0.5, der=1)\n",
    "w1 = ad.autodiff(name=\"w1\", val=0.5, der=1)\n",
    "w2 = ad.autodiff(name=\"w2\", val=-0.5, der=1)\n",
    "\n",
    "\n",
    "f2 = admath.arctan(w0) * (w1/admath.arcsin(x1) - w2/admath.arccos(x2))\n",
    "print('\\n backpropagation evaluation result:', f2.backprop())\n",
    "print('auto differentiate function evaluation result:', f2.der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x1': array([0.70710678, 0.70710678])}\n"
     ]
    }
   ],
   "source": [
    "x1 = ad.autodiff(name=\"x1\", val=[0.5, 0.5], der=1)\n",
    "#x2 = ad.autodiff(name=\"x2\", val=0.5, der=1)\n",
    "w0 = ad.autodiff(name=\"w0\", val=0.5, der=1)\n",
    "w1 = ad.autodiff(name=\"w1\", val=[0.5, -0.5], der=1)\n",
    "#w2 = ad.autodiff(name=\"w2\", val=-0.5, der=1)\n",
    "print(admath.sqrt(x1).der)\n",
    "#print(w1*x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " backpropagation evaluation result: {'w0': 1, 'w1': array([-1.90985932, -1.90985932]), 'x1': array([ 1.82378131, -1.82378131]), 'w2': array([-0.95492966]), 'x2': array([-0.45594533])}\n",
      "auto differentiate function evaluation result: {'w0': array([0.8]), 'w1': array([-1.90985932, -1.90985932]), 'w2': array([-0.95492966]), 'x1': array([ 2.10592126, -2.10592126]), 'x2': array([0.52648031])}\n"
     ]
    }
   ],
   "source": [
    "f6 = admath.arctan(w0)-(w1/admath.arcsin(x1))- w2/admath.arccos(x2)\n",
    "print('\\n backpropagation evaluation result:', f6.backprop())\n",
    "print('auto differentiate function evaluation result:', f6.der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(map(lambda x:x**2>1, [0.5, 0.5]))==True\n",
    "f2 =admath.arcsin(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# questions:\n",
    "- der should be same with backpropagation when function value are the same, or what condition?\n",
    "- when using autodiff math, there no new autodiff object generated and calculated partial derivation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_backpropagation_arc():\n",
    "    '''function for testing backpropagation'''\n",
    "    x1 = ad.autodiff(name=\"x1\", val=0.5, der=1)\n",
    "    x2 = ad.autodiff(name=\"x2\", val=0.5, der=1)\n",
    "    w0 = ad.autodiff(name=\"w0\", val=0.5, der=1)\n",
    "    w1 = ad.autodiff(name=\"w1\", val=0.5, der=1)\n",
    "    w2 = ad.autodiff(name=\"w2\", val=-0.5, der=1)\n",
    "\n",
    "    f = admath.arctan(w0) * (w1/admath.arcsin(x1) - w2/admath.arccos(x2))\n",
    "    assert abs(f.backprop()['w0'] - 1.4323944878270578) < 1E-10\n",
    "    assert abs(f.backprop()['w1'] - 0.8855017059025995) < 1E-10\n",
    "    assert abs(f.backprop()['x1'] - (-0.8455918416642267)) < 1E-10\n",
    "    assert abs(f.backprop()['w2'] - (-0.44275085295129973)) < 1E-10\n",
    "    assert abs(f.backprop()['x2'] - (-0.21139796041605668)) < 1E-10\n",
    "    \n",
    "test_backpropagation_arc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_backpropagation_arc():\n",
    "    '''function for testing backpropagation'''\n",
    "    x1 = ad.autodiff(name=\"x1\", val=[0.5, 0.5], der=1)\n",
    "    w0 = ad.autodiff(name=\"w0\", val=0.5, der=1)\n",
    "    w1 = ad.autodiff(name=\"w1\", val=[0.5, -0.5], der=1)\n",
    "    \n",
    "    f = admath.arctan(w0)-(w1/admath.arcsin(x1))- w2/admath.arccos(x2)\n",
    "    #print('\\n backpropagation evaluation result:', f.backprop())\n",
    "    #print('auto differentiate function evaluation result:', f6.der)\n",
    "    #print(abs(f.backprop()['w1'] - [-1.90985932, -1.90985932]))\n",
    "    assert np.max(abs(f.backprop()['w0'] - 1)) < 1E-6\n",
    "    assert np.max(abs(f.backprop()['w1'] - [-1.90985932, -1.90985932])) < 1E-6\n",
    "    assert np.max(abs(f.backprop()['x1'] - [ 1.82378131, -1.82378131])) < 1E-6\n",
    "    assert np.max(abs(f.backprop()['w2'] - [-0.95492966])) < 1E-6\n",
    "    assert np.max(abs(f.backprop()['x2'] - [-0.45594533])) < 1E-6\n",
    "    \n",
    "test_backpropagation_arc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A= np.matrix([[1, 0],[0, 1]])\n",
    "b= np.matrix([[0, 1]])\n",
    "print(A*b.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "def print(A**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1]\n",
      "2.6\n",
      "[1.5 1.  1.5]\n",
      "2.1050000000000004\n",
      "[1.5 1.  1.5]\n",
      "1.67975\n",
      "[1.5 1.  1.5]\n",
      "1.3142375\n",
      "[1.5 1.  1.5]\n",
      "0.999906875\n",
      "[1.5 1.  1.5]\n",
      "0.7294453437499997\n",
      "[1.5 1.  1.5]\n",
      "0.49660059218749975\n",
      "[1.5 1.  1.5]\n",
      "0.2960253483593748\n",
      "[1.5 1.  1.5]\n",
      "0.24524147253515616\n",
      "[1.5 1.  1.5]\n",
      "0.20845525165488277\n",
      "[0.20845525 0.34867844 0.20845525]\n"
     ]
    }
   ],
   "source": [
    "def updatefun(param, func, data):\n",
    "    #initiate autodiff\n",
    "    w0 = ad.autodiff(name=\"w0\",val = param[0],der = 1)\n",
    "    w1 = ad.autodiff(name=\"w1\", val = param[1], der = 1)\n",
    "    w2 = ad.autodiff(name=\"w2\",val = param[2], der = 1)\n",
    "    \n",
    "    funobj = func(data, w0, w1, w2)\n",
    "    backprop = funobj['backprop']\n",
    "    ypred = funobj['val']\n",
    "    \n",
    "    return backprop, ypred\n",
    "\n",
    "def func(data, w0, w1, w2):\n",
    "    '''\n",
    "    data: 2d array with row as samples, column as features \n",
    "    '''\n",
    "    funobj = {}\n",
    "    vals = []\n",
    "    backprops = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        fobj = data[i, 0]*w0 +data[i, 1]*w1+data[i, 2]*w2\n",
    "        backprops.append(list(fobj.backprop().values()))\n",
    "        vals.append(fobj.val[0])\n",
    "   \n",
    "    funobj['backprop'] = np.mean(backprops, axis = 0) #np.average(backprops, axis = 1)\n",
    "    funobj['val'] = vals\n",
    "    return funobj\n",
    "\n",
    "param = [1, 1, 1]\n",
    "N = 10\n",
    "\n",
    "\n",
    "def grad(param, data,y,  N = 10, tolerance = 0, step = 0.1):\n",
    "    iteration = 0\n",
    "    backprop= np.array([1, 1, 1])\n",
    "    error = 1\n",
    "    while iteration<N and error>tolerance: #number of  1000\n",
    "        #calculate gradient descent\n",
    "        #print(param)\n",
    "        print(backprop)\n",
    "        param = param - step*backprop*param\n",
    "        backprop, ypred = updatefun(param, func, data)\n",
    "        #print(backprop, ypred)\n",
    "        error = np.sum(abs(ypred - y))/len(y)\n",
    "        print(error)\n",
    "        iteration+=1\n",
    "        \n",
    "    return param\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #test on gradient functions\n",
    "    print(grad(np.array([1, 1, 1]), np.array([[1, 1, 1], [2, 1, 2]]), np.array([1, 1]), 10, 1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, 1. , 1.5])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([[1, 1, 1], [2, 1, 2]], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
