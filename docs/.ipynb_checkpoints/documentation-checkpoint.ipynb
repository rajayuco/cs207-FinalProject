{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autodiffpy Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being able to calculate derivatives is crucial for optimization, probabilistic inference, simulations and modeling in various fields such as biology, physics, economics, etc. However, functions used for these sorts of purposes in the real world are often very complex, and it can be quite challenging to calculate the derivatives of these functions in practice. Our automatic differentiation (autodiffpy) software computes the derivatives of any function, with respect to any and all of the function's variables, to machine-precision-level accuracy, by breaking the function down into its elementary operations and using the chain rule (see **Background** for more details).\n",
    "\n",
    "In addition to calculating derivatives, our autodiffpy software can also perform backpropagation. Backpropagation is the process of altering a function's parameters until the outputs of the function behave as expected. For example, given a function that has a set of fixed inputs, a set of weights, and a set of desired outputs, our software can be used to tweak the weights until the output of the function matches the desired output.\n",
    "\n",
    "Our autodiffpy software has many applications, including sensitivity analysis, numerical computation, and machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differenetiation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic differentiation is possible because any function, no matter how complicated, can be represented as a combination of **elementary operations**, such as addition, multiplication, exponentiation, and trigonometry. In other words,  $f(x)$ can be represented as $g_{n}(g_{n-1}(g_{n-2}(...g_1(x))))$, where $g_i(x)$ is the value of the $i^{th}$ elementary operation at x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic differentiation uses the **chain rule** to calculate a function's derivative. Recall that, using the chain rule, the derivative of function $h\\left(u\\left(t\\right)\\right)$ is $\\dfrac{\\partial h}{\\partial t} = \\dfrac{\\partial h}{\\partial u}\\dfrac{\\partial u}{\\partial t}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's say that we want to compute $f^{\\prime}\\left(\\dfrac{\\pi}{16}\\right)$ of a complicated function $f(x)$, where $f'(x)$ denotes the derivative of $f(x)$:\n",
    "$$f\\left(x\\right) = x - \\exp\\left(-2\\sin^{2}\\left(4x\\right)\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation trace below shows how the function $f(x)$ is broken down into combinations of elementary operations. The first column indexes each elementary operation, with the first row representing the value of $x$ itself.  The second column shows the form of each elementary operation, while the third column shows the form of the derivative of each elementary operation.  The fourth column lists the numerical value of each elementary operation and its derivative, respectively.\n",
    "\n",
    "| Trace    | Elementary Operation &nbsp;&nbsp;&nbsp;| Derivative &nbsp;&nbsp;&nbsp; | $\\left(f\\left(a\\right), \\space f^{\\prime}\\left(a\\right)\\right)$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|\n",
    "| :------: | :----------------------:               | :------------------------------: | :------------------------------: |\n",
    "| $x_{1}$  | $\\dfrac{\\pi}{16}$                      | $1$                | $\\left(\\dfrac{\\pi}{16}, 1\\right)$ |\n",
    "| $x_{2}$  | $4x_{1}$                               | $4\\dot{x}_{1}$                 | $\\left(\\dfrac{\\pi}{4}, 4\\right)$ |\n",
    "| $x_{3}$  | $\\sin\\left(x_{2}\\right)$               | $\\cos\\left(x_{2}\\right)\\dot{x}_{2}$            | $\\left(\\dfrac{\\sqrt{2}}{2}, 2\\sqrt{2}\\right)$ |\n",
    "| $x_{4}$  | $x_{3}^{2}$                            | $2x_{3}\\dot{x}_{3}$                   | $\\left(\\dfrac{1}{2}, 4\\right)$ |\n",
    "| $x_{5}$  | $-2x_{4}$                              | $-2\\dot{x}_{4}$ | $\\left(-1, -8\\right)$ |\n",
    "| $x_{6}$  | $\\exp\\left(x_{5}\\right)$               | $\\exp\\left(x_{5}\\right)\\dot{x}_{5}$ | $\\left(\\dfrac{1}{e}, - \\dfrac{8}{e}\\right)$ |\n",
    "| $x_{7}$  | $-x_{6}$                               | $-\\dot{x}_{6}$                  | $\\left(-\\dfrac{1}{e}, \\dfrac{8}{e}\\right)$ |\n",
    "| $x_{8}$  | $x_{1} + x_{7}$                        | $\\dot{x}_{1} + \\dot{x}_{7}$ | $\\left(\\dfrac{\\pi}{16} - \\dfrac{1}{e}, 1 + \\dfrac{8}{e}\\right)$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, $\\space f^{\\prime}\\left(\\dfrac{\\pi}{16}\\right) = 1 + \\frac{8}{e} = 3.9430355293715385. $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **computational graph** drawn below visualizes the evaluation trace. Each node with an incoming arrow represents an elementary operation, which is applied to the node at the tail-end of that same arrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](fig/graph1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our automatic differentiation package uses this approach to calculate the derivatives of a given function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is a special case of automatic differentiation and commonly used by the gradient descent optimization algorithm to adjust the weight of neurons by calculating the gradient of the loss function. The motivation for backpropagation is to train a multi-layered computational process such that it can learn the appropriate internal representations to allow it to learn any arbitrary mapping of input to output in order to update parameters and optimize input vectors. The backpropagation algorithm calculate the differentiation from backwards, which is calculating the partial derivative of the error with respect to a weight $w_{ij}$ is done using the chain rule twice: \n",
    "$${\\displaystyle {\\frac {\\partial E}{\\partial w_{ij}}}={\\frac {\\partial E}{\\partial o_{j}}}{\\frac {\\partial o_{j}}{\\partial {\\text{net}}_{j}}}{\\frac {\\partial {\\text{net}}_{j}}{\\partial w_{ij}}}}$$\n",
    "\n",
    "\n",
    "In order to map the difference of input and output of the function evaluation, we introduce the loss function is a function that maps values of one or more variables onto a real number intuitively representing some \"cost\" associated with those values. For backpropagation, the loss function calculates the difference between the network output and its expected output, after a training example has propagated through the network. The mathematical expression of the loss function must fullfill two conditions in order for it to be possibly used in back propagation. The first is that it can be written as an average over error functions for individual training examples. The reason for this assumption is that the backpropagation algorithm calculates the gradient of the error function for a single training example, which needs to be generalized to the overall error function. The second assumption is that it can be written as a function of the outputs from the neural network.\n",
    "\n",
    "The most common usage of backpropagation is to get the direction for first order iterative optimization algorithms. Gradient descent is a gradient based search algorithm for finding the local minimum for convex mathematical system:\n",
    "\n",
    "$$  {a} _{n+1}={a} _{n}-\\gamma \\nabla F(\\mathbf {a} _{n})$$\n",
    "\n",
    "Convergence to a local minimun is guaranteed and when the function F is convex, all local minima are also global minima, so in this case gradient descent also can converge to the global solultion. Backpropagation will return the $\\nabla F(\\mathbf {a} _{n}) $ for each parameters in order to get new parameter set and update loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x) = g_4(g_3(g_1(x_1, x_2) + g_2(x_2, x_3)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use your package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have distributed our package on PyPI. To install our package, the user can type `pip install autodiffpy` into a terminal command line.\n",
    "\n",
    "Another way for users to download our package is from our github page: https://github.com/rajayuco/cs207-FinalProject.  To do so, users can (1) click the previous link, click the green \"Clone or Download\" button on the website, and then download the zip file manually, OR (2) download the package folder by typing the following into a terminal: `git clone https://github.com/rajayuco/cs207-FinalProject.git`. To install external dependencies, users can navigate into the package directory from within a terminal and type `pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see the **Implementation** section below for examples of using the package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our package is structured as follows:\n",
    "    \n",
    "    -autodiffpy\\\n",
    "         -autodiffpy\\\n",
    "              -__init__.py\n",
    "              -autodiff.py\n",
    "              -autodiff_math.py  \n",
    "         -tests\\\n",
    "              -__init__.py\n",
    "              -autodiff_test.py\n",
    "              -autodiff_math_test.py\n",
    "         -docs\\\n",
    "              -documentation.ipynb\n",
    "         -README.md\n",
    "         -setup.py\n",
    "         -requirements.txt\n",
    "         -LICENSE\n",
    "\n",
    "\n",
    "* Our autodiffpy module is organized as follows:\n",
    "    * autodiff class (autodiff.py)\n",
    "        * Overwrites elementary operations (such as `__add__`, `__pow__`, `__mul__`, `__truediv__`), allowing the calculation of derivatives using automatic differentiation\n",
    "        * Includes the reverse forms of those elementary operations to support both commutative and non-commutative operations (`__radd__`, `__rpow__`, `__rmul__`, `__rtruediv__`, etc.)  \n",
    "        * jacobian method\n",
    "            * Returns an n-dimensional (nd) array representation of the given autodiff instance's derivatives\n",
    "        * backprop method\n",
    "            * Returns the relative error in the inputs of the autodiff instance\n",
    "    * autodiff_math module (autodiff_math.py)\n",
    "        * Contains methods for performing mathematical operations, such as exp(), sinh(), arccos(), tan(), and logistic(), on autodiff instances "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test suite\n",
    "\n",
    "Our test suite is performed through both TravisCI and Coverall.  We make use of both unit testing and doctests, which together break our modules' methods into pieces, subject each piece to a series of tests, and compare the tests' results to what we have declared the output should be.  These tests allow us to detect, in a compartmentalized manner, when new changes to our code cause potential problems.\n",
    "\n",
    "In particular, TravisCI works in parallel with our developed software, as it takes the code we have written and committed to GitHub and runs the test suite that we have defined. So instead of running all tests by hand before deployment, we have been able to focus almost entirely on implementation, meaning we have spent more time building our package and less time checking for structural integrity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The autodiff Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The autodiff class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module **autodiff** contains three components meant for the user: the class *autodiff*, the method *jacobian*, and the method *backprop*.\n",
    "\n",
    "The *autodiff* class allows users to generate variables, such as `x` and `y`, and then use those variables to form a function. The class then performs automatic differentiation on that function.  In the process, the class calculates (1) the numerical value of that equation and (2) the numerical value of that functions’ derivatives with respect to all variables encountered within the function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To carry out this process, the user must first initialize each variable of the desired function separately, as a different instance of the *autodiff* class.\n",
    "\n",
    "Each variable (aka, *autodiff* instance) requires the following inputs:\n",
    "\n",
    "* `name` [string, required]: The name that the user would like to use for this variable (i.e., \"x\" or \"y\").  \n",
    "* `val` [number/nd-array of numbers, required]: The numerical value/nd-array of values that the user would like to assign to this variable.\n",
    "* `der` [number/nd-array of numbers, default=1]: The value/nd-array of this variable’s derivative(s) with respect to itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note that our package assumes implicitly that the user will give each variable a name unique from all other variables, and that all variables will be given values for `val` (and `der`, if specified) that are of the same dimension.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user can then perform mathematical operations on these variables in the form of a function.  Doing so will return a new instance of the *autodiff* class, which will have the following output attributes relevant to the user:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `val` [number/nd-array of numbers]: This returns the computed numerical value of the function.\n",
    "* `der` [dictionary]: This returns a dictionary that contains the values of the function’s derivatives, calculated with respect to every single variable encountered in the function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this returned instance of the *autodiff* class, the user therefore has numerical values/nd-arrays of values for both the function and its derivatives, for all variables encountered within the function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underneath the ‘hood’ of the code, so to speak, the *autodiff* class contains private dunder methods that the user should *not* attempt to access.  These methods override elementary operations (`__add__`, `__sub__`, `__mul__`, `__neg__`, etc.) and the reverse of those operations (`__radd__`, `__rsub__`, `__rmul__`, etc.).  Each overridden method determines the derivatives of the elementary operation, calculated with respect to each unique variable key name contained in the variables’ attribute dictionary `der`.  Each overridden method then returns a new instance of the *autodiff* class, which contains the updated function value/nd-array of values and derivative values/nd-arrays of values stored in its attributes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below demonstrates how users can interact with the *autodiff* class in our software:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> # Import the autodiff class\n",
    ">>> from autodiffpy import autodiff as AD\n",
    ">>> # Create variable instances of the class\n",
    ">>> x = AD.autodiff(name=\"x\", val=[3, 1])\n",
    ">>> y = AD.autodiff(name=\"y\", val=[-4.5, 7])\n",
    ">>> # Define the equation to evaluate\n",
    ">>> f = x**2 + y - x/y\n",
    ">>>\n",
    ">>> # Output the results\n",
    ">>> print(f.val) # Numerical value of equation\n",
    "[ 5.16666667  7.85714286]\n",
    ">>> print(f.der[\"x\"]) # Numerical values of equation’s derivative with respect to \"x\"\n",
    "[ 6.22222222  1.85714286]\n",
    ">>> print(f.der[\"y\"]) # Numerical values of equation’s derivative with respect to \"y\"\n",
    "[ 0.85185185,  0.97959184]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The jacobian method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *jacobian* method within the *autodiff* class allows users to format the derivatives of an instance of the *autodiff* class in `numpy` nd-array form.  Users can specify the variables and the ordering that they would like the nd-array to follow using the input argument `order`.  If `order` is not specified, then the ordering of the returned nd-array will be arbitrary.  *jacobian* returns a dictionary, where the `numpy` nd-array is stored beneath the keyword \"jacobian\", and the ordering is stored beneath the keyword \"order\".\n",
    "\n",
    "The below example, which continues from the previous example, demonstrates the operation of the *jacobian* method:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    ">>> # Print the previously-calculated derivatives in numpy array form (real output won’t have rounded values)\n",
    ">>> f.jacobian() # Dictionary containing formatted nd-array form and ordering\n",
    "{'jacobian': array([[ 6.22222222,  1.85714286],\n",
    "       [ 0.85185185,  0.97959184]]), 'order': ['x', 'y']}\n",
    ">>>\n",
    ">>> # Access the formatted nd-array\n",
    ">>> f.jacobian()[\"jacobian\"] \n",
    "array([[ 6.22222222,  1.85714286],\n",
    "       [ 0.85185185,  0.97959184]])\n",
    ">>>\n",
    ">>> # Access just the derivatives for \"x\"\n",
    ">>> f.jacobian(order=\"x\")[\"jacobian\"] \n",
    "array([[ 6.22222222,  1.85714286]])\n",
    ">>>\n",
    ">>> # Format the derivatives in the order of \"y\", \"x\"\n",
    ">>> f.jacobian(order=[\"y\",\"x\"])[\"jacobian\"] \n",
    "array([[ 0.85185185,  0.97959184],\n",
    "       [ 6.22222222,  1.85714286]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The backprop method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backpropag method allows for the calculation of the change in variables necessary to bring a function's *actual* output closer to the function's *desired* output by using backpropagation. This function is a class method of autodiff, meaning the result will be specific to that autodiff instance.\n",
    "\n",
    "The inputs to this class method are the *desired* values and the loss function. The loss function can be the mean squared error (input = 'MSE'), mean absolute error (input = 'MAE'), root mean squared error (input = 'RMSE'), or mean percentage absolute error (input = 'MPAE'). This function will calculate the respective loss based on the instance's values and the input *desired* values, and also the deriviative of the loss with respect to the instance. Then, backprop will recurse backwards through the computational graph, calculating the derivative of the loss with respect to each instance encountered, until there an instance is reached that has no parent pointers. These instances are the autodiff instances that the user first generated. The backprop method will then return a dictionary containing values for the derivative of the loss with respect to each of the inputted autodiff instances. Coupled with a specified learning rate, the amount that each instance needs to be changed to minimize loss can be calculated.\n",
    "\n",
    "The below example demonstrates a way to use the *backprop* method. 'x' is a list of data with two observations and three features. Autodiff instance 'w' represents a vector of weights for each of the features in 'x'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    ">>>x = [[1,2,3],[0,1,2]]\n",
    ">>>w = autodiff('w', [1,1,1])\n",
    ">>>y_pred = w*x\n",
    ">>>print(y_pred.val)\n",
    "[6,3]\n",
    ">>>y_true = [4,2]\n",
    ">>>print(y_pred.backprop(y_true, 'MSE'))\n",
    "[2,5,8]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward method can utilize the updated weight vector from the backpropagation evaluation for evaluating a new function value and updated loss value. Then the gradient descent method in the autodiff class will call the backprop() method to update weigh for each iteration and then call forward() reevaluate the loss and function value, until converge or reach maximum iteration. A complete example can be shown bellow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>>x = [[1,2,3],[0,1,2]]\n",
    ">>>w = autodiff('w', [1,1,1])\n",
    ">>>y_pred = w*x\n",
    ">>>print(y_pred.val)\n",
    "[6,3]\n",
    ">>>y_true = [4,2]\n",
    ">>>print(y_pred.backprop(y_true, 'MSE'))\n",
    "[2,5,8]\n",
    ">>>print(ad.gradient_descent(f, y_act, beta=beta, loss=loss, max_iter=max_iter, tol=tol))\n",
    ">>>{'f': [4, 2]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The autodiff_math Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "External, elementary mathematical operations, exluding simple arithmetic operations, are included in the *autodiff_math* module. Users are required to import this module to perform these operations on autodiff instances, because `numpy` and other standard math libraries in `python` are not equipped to handle instances of our *autodiff* class.\n",
    "\n",
    "The operations in *autodiff_math* include trigonmetric functions, logarithmic functions, exponential functions, and the logistic function. Each operation within this module returns a new autodiff instance with a properly updated value(s) stored in `val` and an updated dictionary of derivatives stored in `der`.\n",
    "\n",
    "The below example demonstrates how to use our *autodiff_math* module:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Import autodiff\n",
    ">>> from autodiffpy import autodiff as AD\n",
    ">>> from autodiffpy import autodiff_math as adm\n",
    ">>>\n",
    ">>> # Create autodiff instances\n",
    ">>> x = AD.autodiff('x', 100) # Create an autodiff instance\n",
    ">>> y = AD.autodiff('y', 1.5) # Create another autodiff instance\n",
    ">>>\n",
    ">>> # Perform external mathematical operations\n",
    ">>> f1 = adm.log(x, base=10)\n",
    ">>> f2 = adm.sinh(f1*y)\n",
    ">>>\n",
    ">>> # Print the results\n",
    ">>> print(f2.val)\n",
    "[ 10.01787493]\n",
    ">>> print(f2.der)\n",
    "{'x': array([ 0.06558495]), 'y': array([ 20.13532399])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our package requires `numpy` (version 1.15.1), which we use to organize the output of our *jacobian* method and for performing inner math functions (such as $e^x$) within our *autodiff_math* module. Our package also requires `pandas` (version 0.22.0) for taking and dealing with dataframe input in order to change the data format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work for the Future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation of backpropagation (our *backprop* method in the *autodiff* class) calculates the changes in variables necessary to bring a function's actual outputs closer to given desired outputs.  Our implementation calculates these changes efficiently and to machine precision.\n",
    "\n",
    "However, we note that for certain optimization and machine learning applications, such as for neural networks, linear algebra through matrix operations (i.e., through dot product) is extremely useful.\n",
    "\n",
    "In its current form, our backpropagation method, as well as our autodiffpy package as a whole, cannot *directly* perform matrix operations.  There are ways to work around this restriction with our package; linear algebra, for example, is merely a combination of linear equations, and our package is completely equipped to handle linear equations.  But as of yet, our package cannot perform matrix operations.  For now, we leave implementing matrix operations into our package for future work. Also, gradient descent and backpropagation could also be extended to be able to handle cross entropy loss and binary data, and other types of loss function in the future including:\n",
    "$$ Hinge Loss:  \\ell(y) = \\max(0, 1-t \\cdot y) $$\n",
    "\n",
    "$$Cross Engropy Loss: H(T,q)=-\\sum _{{i=1}}^{N}{\\frac  {1}{N}}\\log _{2}q(x_{i})$$\n",
    "\n",
    "$$Huber Loss: L_{\\delta }(a)={\\begin{cases}{\\frac  {1}{2}}{a^{2}}&{\\text{for }}|a|\\leq \\delta ,\\\\\\delta (|a|-{\\frac  {1}{2}}\\delta ),&{\\text{otherwise.}}\\end{cases}}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
