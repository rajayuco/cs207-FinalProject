{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autodiffpy Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being able to calculate derivatives is crucial for optimization, probabilistic inference, simulations and modeling in various fields such as biology, physics, economics, etc. However, functions used for these sorts of purposes in the real world are often very complex, and it can be quite challenging to calculate the derivatives of these functions in practice. Our automatic differentiation (autodiffpy) software computes the derivatives of any function, with respect to any and all of the function's variables, to machine-precision-level accuracy, by breaking the function down into its elementary operations and using the chain rule.\n",
    "\n",
    "In addition to calculating derivatives, our autodiffpy software can also perform gradient descent by utilizing a series of backpropagation and forward propagation. Backpropagation is the process of altering a function's parameters until the outputs of the function behave as expected. For example, given a function that has a set of fixed inputs, a set of weights, and a set of desired outputs, our software can be used to tweak the weights until the output of the function matches the desired output. Forward propagation is the process of recalculating the output values based on the change in weights from backpropagation.\n",
    "\n",
    "Our autodiffpy software has many applications, including sensitivity analysis, numerical computation, and machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differentiation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic differentiation is possible because any function, no matter how complicated, can be represented as a combination of **elementary operations**, such as addition, multiplication, exponentiation, and trigonometry. In other words,  $f(x)$ can be represented as $g_{n}(g_{n-1}(g_{n-2}(...g_1(x))))$, where $g_i(x)$ is the value of the $i^{th}$ elementary operation at x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic differentiation uses the **chain rule** to calculate a function's derivative. Recall that, using the chain rule, the derivative of function $h\\left(u\\left(t\\right)\\right)$ is $\\dfrac{\\partial h}{\\partial t} = \\dfrac{\\partial h}{\\partial u}\\dfrac{\\partial u}{\\partial t}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's say that we want to compute $f^{\\prime}\\left(\\dfrac{\\pi}{16}\\right)$ of a complicated function $f(x)$, where $f'(x)$ denotes the derivative of $f(x)$:\n",
    "$$f\\left(x\\right) = x - \\exp\\left(-2\\sin^{2}\\left(4x\\right)\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation trace below shows how the function $f(x)$ is broken down into combinations of elementary operations. The first column indexes each elementary operation, with the first row representing the value of $x$ itself.  The second column shows the form of each elementary operation, while the third column shows the form of the derivative of each elementary operation.  The fourth column lists the numerical value of each elementary operation and its derivative, respectively.\n",
    "\n",
    "| Trace    | Elementary Operation &nbsp;&nbsp;&nbsp;| Derivative &nbsp;&nbsp;&nbsp; | $\\left(f\\left(a\\right), \\space f^{\\prime}\\left(a\\right)\\right)$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|\n",
    "| :------: | :----------------------:               | :------------------------------: | :------------------------------: |\n",
    "| $x_{1}$  | $\\dfrac{\\pi}{16}$                      | $1$                | $\\left(\\dfrac{\\pi}{16}, 1\\right)$ |\n",
    "| $x_{2}$  | $4x_{1}$                               | $4\\dot{x}_{1}$                 | $\\left(\\dfrac{\\pi}{4}, 4\\right)$ |\n",
    "| $x_{3}$  | $\\sin\\left(x_{2}\\right)$               | $\\cos\\left(x_{2}\\right)\\dot{x}_{2}$            | $\\left(\\dfrac{\\sqrt{2}}{2}, 2\\sqrt{2}\\right)$ |\n",
    "| $x_{4}$  | $x_{3}^{2}$                            | $2x_{3}\\dot{x}_{3}$                   | $\\left(\\dfrac{1}{2}, 4\\right)$ |\n",
    "| $x_{5}$  | $-2x_{4}$                              | $-2\\dot{x}_{4}$ | $\\left(-1, -8\\right)$ |\n",
    "| $x_{6}$  | $\\exp\\left(x_{5}\\right)$               | $\\exp\\left(x_{5}\\right)\\dot{x}_{5}$ | $\\left(\\dfrac{1}{e}, - \\dfrac{8}{e}\\right)$ |\n",
    "| $x_{7}$  | $-x_{6}$                               | $-\\dot{x}_{6}$                  | $\\left(-\\dfrac{1}{e}, \\dfrac{8}{e}\\right)$ |\n",
    "| $x_{8}$  | $x_{1} + x_{7}$                        | $\\dot{x}_{1} + \\dot{x}_{7}$ | $\\left(\\dfrac{\\pi}{16} - \\dfrac{1}{e}, 1 + \\dfrac{8}{e}\\right)$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, $\\space f^{\\prime}\\left(\\dfrac{\\pi}{16}\\right) = 1 + \\frac{8}{e} = 3.9430355293715385. $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **computational graph** drawn below visualizes the evaluation trace. Each node with an incoming arrow represents an elementary operation, which is applied to the node at the tail-end of that same arrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](fig/graph1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our automatic differentiation package uses this approach to calculate the derivatives of a given function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is a special case of automatic differentiation and commonly used by the gradient descent optimization algorithm to adjust the weight of neurons by calculating the gradient of the loss function. The motivation for backpropagation is to train a multi-layered computational process such that it can learn the appropriate internal representations to allow it to learn any arbitrary mapping of input to output in order to update parameters and optimize input vectors. The backpropagation algorithm calculates the derivatives starting from the output moving towards the inputs. The calculation of the partial derivatives of the loss with respect to each weight $w_{ij}$ is done using the chain rule: \n",
    "$${\\displaystyle {\\frac {\\partial L}{\\partial w_{ij}}}={\\frac {\\partial L}{\\partial f_{j}}}{\\frac {\\partial f_{j}}{\\partial {\\text{w}}_{j}}}}$$\n",
    "where L is the loss, and f is an intermediary element in the computational graph.\n",
    "\n",
    "\n",
    "The output of backpropagation is $\\nabla F(\\mathbf {w} _{n})$, which the derivative of the loss with respect to each weight. The most common usage of backpropagation is to obtain estimates for how to change the initial weights in order to reduce loss, through utilizing the following equation:\n",
    "\n",
    "$$  {w} _{n+1}={w} _{n}-\\gamma \\nabla F(\\mathbf {w} _{n})$$\n",
    "\n",
    "where $\\gamma$ is learning rate. With the updated weights, forward propagation (following the computational graph) is performed to obtain the updated value of the output.\n",
    "\n",
    "Gradient descent is the process of sequentially performing backpropagation and forward propagation, until a desired loss is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use our package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have distributed our package on PyPI. To install our package, the user can type `pip install autodiffpy` into a terminal command line.\n",
    "\n",
    "Another way for users to download our package is from our github page: https://github.com/rajayuco/cs207-FinalProject.  To do so, users can (1) click the previous link, click the green \"Clone or Download\" button on the website, and then download the zip file manually, OR (2) download the package folder by typing the following into a terminal: `git clone https://github.com/rajayuco/cs207-FinalProject.git`. To install external dependencies, users can navigate into the package directory from within a terminal and type `pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see the **Implementation** section below for examples of using the package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our package is structured as follows:\n",
    "    \n",
    "    -autodiffpy\\\n",
    "         -autodiffpy\\\n",
    "              -__init__.py\n",
    "              -autodiffmod.py\n",
    "              -autodiff_math.py  \n",
    "         -tests\\\n",
    "              -__init__.py\n",
    "              -autodiff_test.py\n",
    "              -autodiff_math_test.py\n",
    "         -docs\\\n",
    "              -documentation.ipynb\n",
    "              -demo.ipynb\n",
    "         -README.md\n",
    "         -setup.py\n",
    "         -requirements.txt\n",
    "         -LICENSE\n",
    "\n",
    "\n",
    "* Our autodiffpy module is organized as follows:\n",
    "    * autodiff class (autodiff.py)\n",
    "        * Overwrites elementary operations (such as `__add__`, `__pow__`, `__mul__`, `__truediv__`), allowing the calculation of derivatives using automatic differentiation\n",
    "        * Includes the reverse forms of those elementary operations to support both commutative and non-commutative operations (`__radd__`, `__rpow__`, `__rmul__`, `__rtruediv__`, etc.)  \n",
    "        * jacobian method\n",
    "            * Returns an n-dimensional (nd) array representation of the given autodiff instance's derivatives\n",
    "        * gradient descent method\n",
    "            * Utilizes forwardprop and backprop methods\n",
    "    * autodiff_math module (autodiff_math.py)\n",
    "        * Contains methods for performing mathematical operations, such as exp(), sinh(), arccos(), tan(), and logistic(), on autodiff instances "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test suite\n",
    "\n",
    "Our test suite is performed through both TravisCI and Coverall.  We make use of both unit testing and doctests, which together break our modules' methods into pieces, subject each piece to a series of tests, and compare the tests' results to what we have declared the output should be.  These tests allow us to detect, in a compartmentalized manner, when new changes to our code cause potential problems.\n",
    "\n",
    "In particular, TravisCI works in parallel with our developed software, as it takes the code we have written and committed to GitHub and runs the test suite that we have defined. So instead of running all tests by hand before deployment, we have been able to focus almost entirely on implementation, meaning we have spent more time building our package and less time checking for structural integrity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The autodiff Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The autodiff class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module **autodiff** contains autodiff class and gradient descent method.The autodiff class contains jacobian method, backprop method, and forwardprop method.\n",
    "\n",
    "The *autodiff* class allows users to generate variables, such as `x` and `y`, and then use those variables to form a function. The class then performs automatic differentiation on that function.  In the process, the class calculates (1) the numerical value of that equation and (2) the numerical value of that functions’ derivatives with respect to all variables encountered within the function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To carry out this process, the user must first initialize each variable of the desired function separately, as a different instance of the *autodiff* class.\n",
    "\n",
    "Each variable (aka, *autodiff* instance) requires the following inputs:\n",
    "\n",
    "* `name` [string, required]: The name that the user would like to use for this variable (i.e., \"x\" or \"y\").  \n",
    "* `val` [number/nd-array of numbers, required]: The numerical value/nd-array of values that the user would like to assign to this variable.\n",
    "* `der` [number/nd-array of numbers, default=1]: The value/nd-array of this variable’s derivative(s) with respect to itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note that our package assumes implicitly that the user will give each variable a name unique from all other variables.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user can then perform mathematical operations on these variables in the form of a function.  Doing so will return a new instance of the *autodiff* class, which will have the following output attributes relevant to the user:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `val` [number/nd-array of numbers]: This returns the computed numerical value of the function.\n",
    "* `der` [dictionary]: This returns a dictionary that contains the values of the function’s derivatives, calculated with respect to every single variable encountered in the function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this returned instance of the *autodiff* class, the user therefore has numerical values/nd-arrays of values for both the function and its derivatives, for all variables encountered within the function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underneath the ‘hood’ of the code, so to speak, the *autodiff* class contains private dunder methods that the user should *not* attempt to access.  These methods override elementary operations (`__add__`, `__sub__`, `__mul__`, `__neg__`, etc.) and the reverse of those operations (`__radd__`, `__rsub__`, `__rmul__`, etc.).  Each overridden method determines the derivatives of the elementary operation, calculated with respect to each unique variable key name contained in the variables’ attribute dictionary `der`.  Each overridden method then returns a new instance of the *autodiff* class, which contains the updated function value/nd-array of values and derivative values/nd-arrays of values stored in its attributes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below demonstrates how users can interact with the *autodiff* class in our software:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> # Import the autodiff class\n",
    ">>> from autodiffpy import autodiffmod as AD\n",
    ">>> # Create variable instances of the class\n",
    ">>> x = AD.autodiff(name=\"x\", val=[3, 1])\n",
    ">>> y = AD.autodiff(name=\"y\", val=[-4.5, 7])\n",
    ">>> # Define the equation to evaluate\n",
    ">>> f = x**2 + y - x/y\n",
    ">>>\n",
    ">>> # Output the results\n",
    ">>> print(f.val) # Numerical value of equation\n",
    "[ 5.16666667  7.85714286]\n",
    ">>> print(f.der[\"x\"]) # Numerical values of equation’s derivative with respect to \"x\"\n",
    "[ 6.22222222  1.85714286]\n",
    ">>> print(f.der[\"y\"]) # Numerical values of equation’s derivative with respect to \"y\"\n",
    "[1.14814815 1.02040816]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The jacobian method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *jacobian* method within the *autodiff* class allows users to format the derivatives of an instance of the *autodiff* class in `numpy` nd-array form.  Users can specify the variables and the ordering that they would like the nd-array to follow using the input argument `order`.  If `order` is not specified, then the ordering of the returned nd-array will be arbitrary.  *jacobian* returns a dictionary, where the `numpy` nd-array is stored beneath the keyword \"jacobian\", and the ordering is stored beneath the keyword \"order\".\n",
    "\n",
    "The below example, which continues from the previous example, demonstrates the operation of the *jacobian* method:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> # Print the previously-calculated derivatives in numpy array form (real output won’t have rounded values)\n",
    ">>> f.jacobian() # Dictionary containing formatted nd-array form and ordering\n",
    "{'jacobian': array([[6.22222222, 1.85714286],\n",
    "        [1.14814815, 1.02040816]]), 'order': ['x', 'y']}\n",
    "\n",
    ">>> # Access the formatted nd-array\n",
    ">>> f.jacobian()[\"jacobian\"] \n",
    "array([[6.22222222, 1.85714286],\n",
    "       [1.14814815, 1.02040816]])\n",
    "\n",
    ">>> # Access just the derivatives for \"x\"\n",
    ">>> f.jacobian(order=\"x\")[\"jacobian\"] \n",
    "array([[ 6.22222222,  1.85714286]])\n",
    "\n",
    ">>> # Format the derivatives in the order of \"y\", \"x\"\n",
    ">>> f.jacobian(order=[\"y\",\"x\"])[\"jacobian\"] \n",
    "array([[1.14814815, 1.02040816],\n",
    "       [6.22222222, 1.85714286]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The backprop method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backpropag method allows for the calculation of the change in variables necessary to bring a function's *actual* output closer to the function's *desired* output by using backpropagation. This function is a class method of autodiff, meaning the result will be specific to that autodiff instance.\n",
    "\n",
    "The inputs to this class method are the *desired* values and the loss function. The loss function can be the mean squared error (input = 'MSE'), mean absolute error (input = 'MAE'), root mean squared error (input = 'RMSE'), or mean percentage absolute error (input = 'MPAE'). This function will calculate the respective loss based on the instance's values and the input *desired* values, and also the deriviative of the loss with respect to the instance. Then, backprop will recurse backwards through the computational graph, calculating the derivative of the loss with respect to each instance encountered, until there an instance is reached that has no parent pointers. These instances are the autodiff instances that the user first generated. The backprop method will then return a dictionary containing values for the derivative of the loss with respect to each of the inputted autodiff instances. Coupled with a specified learning rate, the amount that each instance needs to be changed to minimize loss can be calculated.\n",
    "\n",
    "The below example demonstrates a way to use the *backprop* method. 'x' is a list of data with two observations and three features. Autodiff instance 'w' represents a vector of weights for each of the features in 'x'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    ">>>x = [[1,2,3],[0,1,2]]\n",
    ">>>w = autodiff('w', [1,1,1])\n",
    ">>>y_pred = w*x\n",
    ">>>print(y_pred.val)\n",
    "[6,3]\n",
    ">>>y_true = [4,2]\n",
    ">>>print(y_pred.backprop(y_true, 'MSE'))\n",
    "[2,5,8]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The gradient descent and forwardprop methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward method can utilize the updated weight vector from the backpropagation evaluation for evaluating a new function value and updated loss value. Then the gradient descent method in the autodiff class will call the backprop() method to update weigh for each iteration and then call forward() reevaluate the loss and function value, until converge or reach maximum iteration. A complete example can be shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>>x = [[1,2,3],[0,1,2]]\n",
    ">>>w = autodiff('w', [1,1,1])\n",
    ">>>y_pred = w*x\n",
    ">>>print(y_pred.val)\n",
    "[6,3]\n",
    ">>>y_true = [4,2]\n",
    ">>>print(y_pred.backprop(y_true, 'MSE'))\n",
    "[2,5,8]\n",
    ">>>print(ad.gradient_descent(f, y_act, beta=beta, loss=loss, max_iter=max_iter, tol=tol))\n",
    ">>>{'f': [4, 2]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The autodiff_math Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "External, elementary mathematical operations, exluding simple arithmetic operations, are included in the *autodiff_math* module. Users are required to import this module to perform these operations on autodiff instances, because `numpy` and other standard math libraries in `python` are not equipped to handle instances of our *autodiff* class.\n",
    "\n",
    "The operations in *autodiff_math* include trigonmetric functions, logarithmic functions, exponential functions, and the logistic function. Each operation within this module returns a new autodiff instance with a properly updated value(s) stored in `val` and an updated dictionary of derivatives stored in `der`.\n",
    "\n",
    "The below example demonstrates how to use our *autodiff_math* module:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Import autodiff\n",
    ">>> from autodiffpy import autodiffmod as AD\n",
    ">>> from autodiffpy import autodiff_math as adm\n",
    ">>>\n",
    ">>> # Create autodiff instances\n",
    ">>> x = AD.autodiff('x', 100) # Create an autodiff instance\n",
    ">>> y = AD.autodiff('y', 1.5) # Create another autodiff instance\n",
    ">>>\n",
    ">>> # Perform external mathematical operations\n",
    ">>> f1 = adm.log(x, base=10)\n",
    ">>> f2 = adm.sinh(f1*y)\n",
    ">>>\n",
    ">>> # Print the results\n",
    ">>> print(f2.val)\n",
    "[ 10.01787493]\n",
    ">>> print(f2.der)\n",
    "{'x': array([ 0.06558495]), 'y': array([ 20.13532399])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our package requires `numpy` (version 1.15.1), which we use to organize the output of our *jacobian* method and for performing inner math functions (such as $e^x$) within our *autodiff_math* module. Our package also requires `pandas` (version 0.22.0) for taking and dealing with dataframe input in order to change the data format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. When performing gradient descent function, the weights must be stored in autodiff instance where the name must be 'w'. \n",
    "\n",
    "```python\n",
    ">>> w = autodiff('w', [1,1,1]) # correct way to initiate weights\n",
    ">>> w = autodiff('t', [1,1,1]) # incorrect\n",
    "```\n",
    "\n",
    "2. When multiplying autodiff instance by `numpy array` or `pandas DataFrame`, the autodiff instance must be the first term in the multiplication. Otherwise, numpy or pandas libraries will send individual elements to multiplication, rather than the entire array/dataframe.\n",
    "\n",
    "\n",
    "3. If an autodiff instance has a matrix value, and is multiplied by a matrix of a different size, the resulting autodiff instance cannot be the denominator of any division. This is because the derivative and value of the resulting autodiff instance will have different matrix shapes.\n",
    "\n",
    "\n",
    "4. Gradient descent currently only works with a narrow range of values for data and weight estimates. This is due to a non-optimized adaptive learning rate algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. We plan to fix current limitations mentioned above.\n",
    "\n",
    "\n",
    "2. Our package currently does not perform matrix multiplication when the matrices are the same size; rather, our package does element-wise multiplications in these instances. For now, we leave implementing matrix operations into our package for future work. \n",
    "\n",
    "\n",
    "3. Gradient descent and backpropagation could also be extended to be able to handle cross entropy loss and binary data, and other types of loss function in the future including:\n",
    "\n",
    "$$ Hinge Loss:  \\ell(y) = \\max(0, 1-t \\cdot y) $$\n",
    "\n",
    "$$Cross Engropy Loss: H(T,q)=-\\sum _{{i=1}}^{N}{\\frac  {1}{N}}\\log _{2}q(x_{i})$$\n",
    "\n",
    "$$Huber Loss: L_{\\delta }(a)={\\begin{cases}{\\frac  {1}{2}}{a^{2}}&{\\text{for }}|a|\\leq \\delta ,\\\\\\delta (|a|-{\\frac  {1}{2}}\\delta ),&{\\text{otherwise.}}\\end{cases}}$$\n",
    "\n",
    "\n",
    "4. We would like to improve our gradient descent algorithm so that we have an appropriate adaptive learning rate that can handle a wide arrange of inputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
